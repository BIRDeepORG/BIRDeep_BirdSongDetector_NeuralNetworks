{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Ground Truths:  542\n"
     ]
    }
   ],
   "source": [
    "# Ground Truth CSV\n",
    "PATH = \"../Data/\"\n",
    "gt_csv = PATH + \"Dataset/CSVs/test_with_bg.csv\"\n",
    "\n",
    "# Read the Ground Truth CSV\n",
    "gt_df = pd.read_csv(gt_csv)\n",
    "print(\"Número de Ground Truths: \", len(gt_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to group predictions and ground truth annotations (if same specie and overlapping times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_consecutive_predictions_birdnet(predictions_df):\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    for _, row in predictions_df.iterrows():\n",
    "        if current_group and (row['Start (s)'] != last_end or row['Scientific name'] != current_group[-1]['Scientific name']):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['End (s)']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "    # Combine groups in unique predictions\n",
    "    combined_predictions = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'Start (s)': group[0]['Start (s)'],\n",
    "            'End (s)': group[-1]['End (s)'],\n",
    "            'Scientific name': group[0]['Scientific name'],\n",
    "            'Confidence': max(item['Confidence'] for item in group)  # conf = max confidence in group\n",
    "        }\n",
    "        combined_predictions.append(combined_prediction)\n",
    "    return combined_predictions\n",
    "\n",
    "def group_consecutive_annotations_gt(annotations_df):\n",
    "    annotations_df.sort_values(by=['path', 'specie', 'start_time'], inplace=True)\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    last_path = None\n",
    "    last_specie = None\n",
    "\n",
    "    for _, row in annotations_df.iterrows():\n",
    "        if current_group and (row['path'] != last_path or row['specie'] != last_specie or row['start_time'] - last_end >= 2):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['end_time']\n",
    "        last_path = row['path']\n",
    "        last_specie = row['specie']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "\n",
    "    # Combine groups into unique predictions\n",
    "    combined_annotations = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'path': group[0]['path'],\n",
    "            'start_time': group[0]['start_time'],\n",
    "            'end_time': group[-1]['end_time'],\n",
    "            'specie': group[0]['specie'],\n",
    "        }\n",
    "        combined_annotations.append(combined_prediction)\n",
    "    \n",
    "    return combined_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Ground Truths agrupados:  403\n"
     ]
    }
   ],
   "source": [
    "gt_df = group_consecutive_annotations_gt(gt_df)\n",
    "gt_df = pd.DataFrame(gt_df)  # Convert list dict to DataFrame\n",
    "print(\"Número de Ground Truths agrupados: \", len(gt_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the IoU\n",
    "def calculate_iou(interval1, interval2):\n",
    "    start_max = max(interval1[0], interval2[0])\n",
    "    end_min = min(interval1[1], interval2[1])\n",
    "    intersection = max(0, end_min - start_max)\n",
    "    union = (interval1[1] - interval1[0]) + (interval2[1] - interval2[0]) - intersection\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_detection_birdnet(gt_row, grouped_predictions, confidence_threshold, iou_threshold=0):\n",
    "    for prediction in grouped_predictions:\n",
    "        if prediction['Confidence'] >= confidence_threshold:\n",
    "            iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_row['start_time'], gt_row['end_time']))\n",
    "            if iou >= iou_threshold:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def is_detection_classification_birdnet(gt_row, grouped_predictions, confidence_threshold, iou_threshold=0):\n",
    "    for prediction in grouped_predictions:\n",
    "        if prediction['Confidence'] >= confidence_threshold:\n",
    "            iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_row['start_time'], gt_row['end_time']))\n",
    "            if (prediction['Scientific name'].lower() == gt_row['specie'].lower() and\n",
    "                iou >= iou_threshold):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(total_predictions, prediction_conf_score, total_predictions_score, total_gt, true_positives_detector, true_positives_classifier, correct_predictions, false_positives, false_negatives_detector, false_negatives_classifier, true_negatives):\n",
    "    print(\"================== Metrics ==================\\n\")\n",
    "    print(f\"Total Predictions: {total_predictions}\")\n",
    "    print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "    print(f\"Total GT: {total_gt}\")\n",
    "    print(f\"Correct Predictions Detector: {true_positives_detector}\")\n",
    "    print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "    # Additional calculations for precision, recall, and F1-score\n",
    "    print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    accuracy_detector = (true_positives_detector + true_negatives) / (true_positives_detector + true_negatives + false_positives + false_negatives_detector) if true_positives_detector + true_negatives + false_positives + false_negatives_detector != 0 else 0\n",
    "    precision_detector = true_positives_detector / (true_positives_detector + false_positives) if true_positives_detector + false_positives != 0 else 0\n",
    "    recall_detector = true_positives_detector / (true_positives_detector + false_negatives_detector) if true_positives_detector + false_negatives_detector != 0 else 0\n",
    "    f1_score_detector = 2 * precision_detector * recall_detector / (precision_detector + recall_detector) if precision_detector + recall_detector != 0 else 0\n",
    "    print(f\"Accuracy: {accuracy_detector}\")\n",
    "    print(f\"Precision: {precision_detector}\")\n",
    "    print(f\"Recall: {recall_detector}\")\n",
    "    print(f\"F1-Score: {f1_score_detector}\")\n",
    "\n",
    "    print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    accuracy_classifier = (true_positives_classifier + true_negatives) / (true_positives_classifier + true_negatives + false_positives + false_negatives_detector) if true_positives_classifier + true_negatives + false_positives + false_negatives_classifier != 0 else 0\n",
    "    precision_classifier = true_positives_classifier / (true_positives_classifier + false_positives) if true_positives_classifier + false_positives != 0 else 0\n",
    "    recall_classifier = true_positives_classifier / (true_positives_classifier + false_negatives_detector) if true_positives_classifier + false_negatives_detector != 0 else 0\n",
    "    f1_score_classifier = 2 * precision_classifier * recall_classifier / (precision_classifier + recall_classifier) if precision_classifier + recall_classifier != 0 else 0\n",
    "    print(f\"Accuracy: {accuracy_classifier}\")\n",
    "    print(f\"Precision: {precision_classifier}\")\n",
    "    print(f\"Recall: {recall_classifier}\")\n",
    "    print(f\"F1-Score: {f1_score_classifier}\")\n",
    "\n",
    "    print(\"\\n================== Other ==================\\n\")\n",
    "    print(f\"False Positives: {false_positives}\")\n",
    "\n",
    "def analyze_predictions_BirdNET(analysis_name, prediction_conf_score=0.6, iou_threshold=0.1):\n",
    "    # Variables for metrics\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    total_predictions_score = 0\n",
    "\n",
    "    total_gt = len(gt_df)\n",
    "\n",
    "    # Metrics\n",
    "    true_positives_detector = 0\n",
    "    true_positives_classifier = 0\n",
    "    false_positives = 0\n",
    "    false_negatives_detector = 0\n",
    "    false_negatives_classifier = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    last_file = None\n",
    "\n",
    "    # Process the predictions\n",
    "    for _, gt_annotation in gt_df.iterrows():\n",
    "        # Load the predictions\n",
    "        prediction_path = f\"../BirdNET/Predictions/{analysis_name}/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "\n",
    "        current_file = gt_annotation['path']\n",
    "\n",
    "        try:\n",
    "            predictions_df = pd.read_csv(prediction_path)\n",
    "            grouped_predictions = group_consecutive_predictions_birdnet(predictions_df)  # Group predictions\n",
    "\n",
    "            if current_file != last_file:\n",
    "                total_predictions += len(grouped_predictions)\n",
    "                current_predictions_score = len([p for p in grouped_predictions if p['Confidence'] >= prediction_conf_score])\n",
    "                total_predictions_score += current_predictions_score\n",
    "\n",
    "            # If the annotation if not a background\n",
    "            if gt_annotation['specie'] != 'No audio':\n",
    "                # Check if the GT is detected by the detector\n",
    "                if is_detection_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    true_positives_detector += 1\n",
    "                else:\n",
    "                    false_negatives_detector += 1\n",
    "                \n",
    "                if is_detection_classification_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                    true_positives_classifier += 1\n",
    "                else:\n",
    "                    false_negatives_classifier += 1\n",
    "            else: # Annotation is a background\n",
    "                # Take only grouped_predictions with confidence >= prediction_conf_score\n",
    "                if current_predictions_score == 0:\n",
    "                    true_negatives += 1\n",
    "                else: # Background is unique file, with no annotations, so all predictions on that file are false positives\n",
    "                    false_positives += current_predictions_score\n",
    "\n",
    "            last_file = current_file\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    display_metrics(total_predictions, prediction_conf_score, total_predictions_score, total_gt, true_positives_detector, true_positives_classifier, correct_predictions, false_positives, false_negatives_detector, false_negatives_classifier, true_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions_YOLO_BirdNET(analysis_name, prediction_conf_score=0.6, iou_threshold=0.1):\n",
    "    # Variables for metrics\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    total_predictions_score = 0\n",
    "    total_gt = len(gt_df)\n",
    "    # Metrics\n",
    "    true_positives_detector = 0\n",
    "    true_positives_classifier = 0\n",
    "    false_positives = 0\n",
    "    false_negatives_detector = 0\n",
    "    false_negatives_classifier = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    last_file = None\n",
    "\n",
    "    # Process the predictions\n",
    "    for _, gt_annotation in gt_df.iterrows():\n",
    "        gt_basename = gt_annotation['path'].split('/')[-1]\n",
    "        gt_basename_no_ext = os.path.splitext(gt_basename)[0]\n",
    "        # Assuming the structure of the file name is consistent with the example given\n",
    "        prediction_files = os.listdir(f\"../BirdNET/Predictions/{analysis_name}\")\n",
    "        prediction_files_filtered = [file for file in prediction_files if file.startswith(gt_basename_no_ext)]\n",
    "\n",
    "        current_file = gt_annotation['path']\n",
    "        \n",
    "        # predictions_df initialization\n",
    "        predictions_df = pd.DataFrame(columns=['Start (s)', 'End (s)', 'Scientific name', 'Common name', 'Confidence'])\n",
    "\n",
    "        for prediction_file in prediction_files_filtered:\n",
    "            start_time, end_time = prediction_file.split('_')[-2:]\n",
    "            end_time = end_time.split('.BirdNET')[0]\n",
    "            start_time = float(start_time)\n",
    "            end_time = float(end_time)\n",
    "            prediction_path = f\"../BirdNET/Predictions/{analysis_name}/{prediction_file}\"\n",
    "\n",
    "            # Create prediction DataFrame and append all of current prediction file\n",
    "            current_prediction_df = pd.read_csv(prediction_path)\n",
    "\n",
    "            # Set start and end times\n",
    "            current_prediction_df['Start (s)'] = float(start_time)\n",
    "            current_prediction_df['End (s)'] = float(end_time)\n",
    "\n",
    "            # If current_prediction_df is not empty, concatenate it to predictions_df\n",
    "            if not current_prediction_df.empty:\n",
    "                if predictions_df.empty:\n",
    "                    predictions_df = current_prediction_df\n",
    "                else:\n",
    "                    predictions_df = pd.concat([predictions_df, current_prediction_df], ignore_index=True)\n",
    "\n",
    "        grouped_predictions = group_consecutive_predictions_birdnet(predictions_df)  # Group predictions\n",
    "        \n",
    "        try:\n",
    "            if current_file != last_file:\n",
    "                total_predictions += len(grouped_predictions)\n",
    "                current_predictions_score = len([p for p in grouped_predictions if p['Confidence'] >= prediction_conf_score])\n",
    "                total_predictions_score += current_predictions_score\n",
    "            # If the annotation if not a background\n",
    "            if gt_annotation['specie'] != 'No audio':\n",
    "                # Check if the GT is detected by the detector\n",
    "                if is_detection_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    true_positives_detector += 1\n",
    "                else:\n",
    "                    false_negatives_detector += 1\n",
    "                \n",
    "                if is_detection_classification_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                    true_positives_classifier += 1\n",
    "                else:\n",
    "                    false_negatives_classifier += 1\n",
    "            else: # Annotation is a background\n",
    "                # Take only grouped_predictions with confidence >= prediction_conf_score\n",
    "                if current_predictions_score == 0:\n",
    "                    true_negatives += 1\n",
    "                else: # Background is unique file, with no annotations, so all predictions on that file are false positives\n",
    "                    false_positives += current_predictions_score\n",
    "            \n",
    "            last_file = current_file\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    display_metrics(total_predictions, prediction_conf_score, total_predictions_score, total_gt, true_positives_detector, true_positives_classifier, correct_predictions, false_positives, false_negatives_detector, false_negatives_classifier, true_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Base con la lista de especies de Doñana de BIRDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 194\n",
      "Total Predictions with Score >= 0.6: 27\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 35\n",
      "Correct Predictions Detector + Classifier: 9\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.11940298507462686\n",
      "Precision: 1.0\n",
      "Recall: 0.078125\n",
      "F1-Score: 0.14492753623188406\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.06772009029345373\n",
      "Precision: 1.0\n",
      "Recall: 0.02132701421800948\n",
      "F1-Score: 0.04176334106728538\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 0\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"0_BirdNet_Base_AllTest_DonanaSpecies\", 0.6, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Base con la lista de especies del customClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 160\n",
      "Total Predictions with Score >= 0.6: 39\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 48\n",
      "Correct Predictions Detector + Classifier: 19\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.14498933901918976\n",
      "Precision: 0.9795918367346939\n",
      "Recall: 0.10714285714285714\n",
      "F1-Score: 0.19315895372233396\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.08863636363636364\n",
      "Precision: 0.95\n",
      "Recall: 0.045346062052505964\n",
      "F1-Score: 0.08656036446469248\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 1\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"1_BirdNet_Base_AllTest_ClassifierSpecies\", 0.6, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 160\n",
      "Total Predictions with Score >= 0.1: 160\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 119\n",
      "Correct Predictions Detector + Classifier: 54\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.2902542372881356\n",
      "Precision: 0.952\n",
      "Recall: 0.265625\n",
      "F1-Score: 0.41535776614310643\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.1769041769041769\n",
      "Precision: 0.9\n",
      "Recall: 0.1409921671018277\n",
      "F1-Score: 0.24379232505643345\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 6\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"1_BirdNet_Base_AllTest_ClassifierSpecies\", 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Fine Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 527\n",
      "Total Predictions with Score >= 0.6: 10\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 11\n",
      "Correct Predictions Detector + Classifier: 5\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.06823027718550106\n",
      "Precision: 1.0\n",
      "Recall: 0.024553571428571428\n",
      "F1-Score: 0.04793028322440087\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.056155507559395246\n",
      "Precision: 1.0\n",
      "Recall: 0.011312217194570135\n",
      "F1-Score: 0.02237136465324385\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 0\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"2_BirdNet_FineTuning_AllTest\", 0.6, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 527\n",
      "Total Predictions with Score >= 0.1: 527\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 215\n",
      "Correct Predictions Detector + Classifier: 85\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.45708582834331335\n",
      "Precision: 0.8464566929133859\n",
      "Recall: 0.4799107142857143\n",
      "F1-Score: 0.6125356125356126\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.2668463611859838\n",
      "Precision: 0.6854838709677419\n",
      "Recall: 0.2672955974842767\n",
      "F1-Score: 0.3846153846153846\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 39\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"2_BirdNet_FineTuning_AllTest\", 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baja los scores de las clases con las que ha sido entrenado, teniendo que bajar la threshold mucho. Haciendo que haya muchos más Falsos Positivos (en los casos anteriores con 0.6 -> 0, 6... FP, ahora hay que bajar hasta 0.1 el accuracy es mejor pero tenemos muchos Falsos Positivos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET customClassifier con los recortes de YOLOv8 para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 79\n",
      "Total Predictions with Score >= 0.2: 37\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 15\n",
      "Correct Predictions Detector + Classifier: 7\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.0767590618336887\n",
      "Precision: 1.0\n",
      "Recall: 0.033482142857142856\n",
      "F1-Score: 0.06479481641468683\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.06073752711496746\n",
      "Precision: 1.0\n",
      "Recall: 0.015909090909090907\n",
      "F1-Score: 0.03131991051454139\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 0\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_YOLO_BirdNET(\"3_BirdNET_FineTuning_DetectorTest\", 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 406\n",
      "Number of annotations: 403\n",
      "TP: 273, FP: 17, FN: 133, TN: 0\n",
      "Accuracy: 0.6453900709219859\n",
      "Precision: 0.9413793103448276, Recall: 0.6724137931034483, F1-Score: 0.7844827586206896\n"
     ]
    }
   ],
   "source": [
    "# Umbral para considerar una predicción como correcta\n",
    "iou_threshold = 0.1\n",
    "experiment = \"predict_conf01_iou02\"\n",
    "\n",
    "# Leer el archivo CSV con anotaciones\n",
    "annotations_df = pd.read_csv('../BirdNET/Dataset/test_with_bg.csv')\n",
    "\n",
    "annotations_df = group_consecutive_annotations_gt(annotations_df)\n",
    "annotations_df = pd.DataFrame(gt_df)  # Convert list dict to DataFrame\n",
    "\n",
    "# Listar archivos de predicciones\n",
    "prediction_files = os.listdir(f'../BirdNET/Audios/{experiment}')\n",
    "\n",
    "number_of_predictions = len(prediction_files)\n",
    "print(f\"Number of predictions: {number_of_predictions}\")\n",
    "print(f\"Number of annotations: {len(annotations_df)}\")\n",
    "\n",
    "# Initialize an empty list to store prediction data\n",
    "predictions_data = []\n",
    "\n",
    "# Convert prediction files into a list of dictionaries with basename, start_time, end_time\n",
    "for prediction in prediction_files:\n",
    "    parts = prediction.split('_')\n",
    "    basename = '_'.join(parts[:3])\n",
    "    start_time = parts[-2]\n",
    "    end_time = parts[-1].split('.W')[0]\n",
    "\n",
    "    predictions_data.append({'basename': basename, 'start_time': start_time, 'end_time': end_time})\n",
    "\n",
    "    # predictions_interval = [start_time, end_time]\n",
    "    \n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_data, columns=['basename', 'start_time', 'end_time'])\n",
    "\n",
    "# Inicializar contadores\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "\n",
    "# Initialize lists for storing FP and FN predictions\n",
    "fp_predictions = []\n",
    "fn_predictions = []\n",
    "\n",
    "# change annotations_df path to basename\n",
    "annotations_df['basename'] = annotations_df['path'].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "# Identificar anotaciones \"No audio\"\n",
    "no_audio_annotations = annotations_df[annotations_df['specie'] == 'No audio']\n",
    "\n",
    "# If there are basename of predictions that match basename of no_audio_annotations, they are false positive\n",
    "if not no_audio_annotations.empty:\n",
    "    for _, row in no_audio_annotations.iterrows():\n",
    "        if row['basename'] in predictions_df['basename'].values:\n",
    "            FP += len(predictions_df[predictions_df['basename'] == row['basename']])\n",
    "            # Append FP predictions\n",
    "            for _, prediction in predictions_df[predictions_df['basename'] == row['basename']].iterrows():\n",
    "                filename = str(prediction['basename'] + \"_\" + str(prediction[\"start_time\"]) + \"_\" + str(prediction[\"end_time\"]))\n",
    "                fp_predictions.append({'file': filename, 'reason': 'Predicted audio in no-audio segment'})\n",
    "        else:\n",
    "            TN += len(predictions_df[predictions_df['basename'] == row['basename']])\n",
    "\n",
    "# Eliminate empty audios from annotations_df\n",
    "annotations_df = annotations_df[annotations_df['specie'] != 'No audio']\n",
    "\n",
    "# Loop through all predictions\n",
    "for _, prediction in predictions_df.iterrows():\n",
    "    matched = False\n",
    "    for _, row in annotations_df.iterrows():\n",
    "        annotation_interval = [row['start_time'], row['end_time']]\n",
    "        prediction_interval = [float(prediction['start_time']), float(prediction['end_time'])]\n",
    "        iou = calculate_iou(prediction_interval, annotation_interval)\n",
    "        if iou >= iou_threshold and prediction['basename'] == row['basename']:\n",
    "            TP += 1\n",
    "            matched = True\n",
    "            break\n",
    "    if not matched:\n",
    "        FN += 1\n",
    "        filename = str(prediction['basename'] + \"_\" + str(prediction[\"start_time\"]) + \"_\" + str(prediction[\"end_time\"]))\n",
    "        fn_predictions.append({'file': filename, 'reason': 'No matching annotation found'})\n",
    "    \n",
    "# Convert lists to DataFrames\n",
    "fp_df = pd.DataFrame(fp_predictions)\n",
    "fn_df = pd.DataFrame(fn_predictions)\n",
    "\n",
    "# Calcular métricas de rendimiento\n",
    "accuracy = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN > 0 else 0\n",
    "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "print(f\"TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AM11_20230530_102000_0.00_28.60</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AM11_20230530_102000_0.00_3.13</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AM1_20230511_063000_19.99_20.72</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AM1_20230511_063000_20.03_20.76</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AM1_20230511_063000_23.38_27.22</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AM1_20230511_063000_23.45_26.37</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AM1_20230511_063000_44.89_46.78</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AM1_20230511_063000_53.42_56.47</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AM1_20230511_063000_58.93_60.00</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AM1_20230511_063000_59.00_59.98</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AM1_20230511_103000_0.21_2.26</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AM1_20230511_103000_40.72_42.48</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AM1_20230511_103000_49.06_50.60</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AM1_20230511_103000_8.11_10.22</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AM1_20230530_073000_0.09_2.04</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AM4_20230531_092000_0.02_1.76</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AM4_20230531_092000_39.50_41.28</td>\n",
       "      <td>Predicted audio in no-audio segment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               file                               reason\n",
       "13  AM11_20230530_102000_0.00_28.60  Predicted audio in no-audio segment\n",
       "14   AM11_20230530_102000_0.00_3.13  Predicted audio in no-audio segment\n",
       "0   AM1_20230511_063000_19.99_20.72  Predicted audio in no-audio segment\n",
       "7   AM1_20230511_063000_20.03_20.76  Predicted audio in no-audio segment\n",
       "2   AM1_20230511_063000_23.38_27.22  Predicted audio in no-audio segment\n",
       "6   AM1_20230511_063000_23.45_26.37  Predicted audio in no-audio segment\n",
       "5   AM1_20230511_063000_44.89_46.78  Predicted audio in no-audio segment\n",
       "1   AM1_20230511_063000_53.42_56.47  Predicted audio in no-audio segment\n",
       "4   AM1_20230511_063000_58.93_60.00  Predicted audio in no-audio segment\n",
       "3   AM1_20230511_063000_59.00_59.98  Predicted audio in no-audio segment\n",
       "9     AM1_20230511_103000_0.21_2.26  Predicted audio in no-audio segment\n",
       "11  AM1_20230511_103000_40.72_42.48  Predicted audio in no-audio segment\n",
       "8   AM1_20230511_103000_49.06_50.60  Predicted audio in no-audio segment\n",
       "10   AM1_20230511_103000_8.11_10.22  Predicted audio in no-audio segment\n",
       "12    AM1_20230530_073000_0.09_2.04  Predicted audio in no-audio segment\n",
       "15    AM4_20230531_092000_0.02_1.76  Predicted audio in no-audio segment\n",
       "16  AM4_20230531_092000_39.50_41.28  Predicted audio in no-audio segment"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort alphabetically by file name\n",
    "fp_df.sort_values(by='file', inplace=True)\n",
    "# fn_df.sort_values(by='file', inplace=True)\n",
    "\n",
    "fp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia a la carpeta FP los archivos de audio que son falsos positivos\n",
    "for _, row in fp_df.iterrows():\n",
    "    file = row['file']\n",
    "    src = f'../BirdNET/Audios/{experiment}/{file}.WAV'\n",
    "    dst = f'../BirdNET/FP/{experiment}/{file}.WAV'\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "    os.system(f'cp {src} {dst}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 193\n",
      "Number of annotations: 403\n",
      "TP: 149, FP: 7, FN: 44, TN: 0\n",
      "Accuracy: 0.745\n",
      "Precision: 0.9551282051282052, Recall: 0.772020725388601, F1-Score: 0.8538681948424068\n"
     ]
    }
   ],
   "source": [
    "# Umbral para considerar una predicción como correcta\n",
    "iou_threshold = 0.1\n",
    "experiment = \"predict_conf02_iou02\"\n",
    "\n",
    "# Leer el archivo CSV con anotaciones\n",
    "annotations_df = pd.read_csv('../BirdNET/Dataset/test_with_bg.csv')\n",
    "\n",
    "annotations_df = group_consecutive_annotations_gt(annotations_df)\n",
    "annotations_df = pd.DataFrame(gt_df)  # Convert list dict to DataFrame\n",
    "\n",
    "# Listar archivos de predicciones\n",
    "prediction_files = os.listdir(f'../BirdNET/Audios/{experiment}')\n",
    "\n",
    "number_of_predictions = len(prediction_files)\n",
    "print(f\"Number of predictions: {number_of_predictions}\")\n",
    "print(f\"Number of annotations: {len(annotations_df)}\")\n",
    "\n",
    "# Initialize an empty list to store prediction data\n",
    "predictions_data = []\n",
    "\n",
    "# Convert prediction files into a list of dictionaries with basename, start_time, end_time\n",
    "for prediction in prediction_files:\n",
    "    parts = prediction.split('_')\n",
    "    basename = '_'.join(parts[:3])\n",
    "    start_time = float(parts[-2])\n",
    "    end_time = float(parts[-1].split('.W')[0])\n",
    "\n",
    "    predictions_data.append({'basename': basename, 'start_time': start_time, 'end_time': end_time})\n",
    "\n",
    "    # predictions_interval = [start_time, end_time]\n",
    "    \n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_data, columns=['basename', 'start_time', 'end_time'])\n",
    "\n",
    "# Inicializar contadores\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "\n",
    "# Initialize lists for storing FP and FN predictions\n",
    "fp_predictions = []\n",
    "fn_predictions = []\n",
    "\n",
    "# change annotations_df path to basename\n",
    "annotations_df['basename'] = annotations_df['path'].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
    "\n",
    "# Identificar anotaciones \"No audio\"\n",
    "no_audio_annotations = annotations_df[annotations_df['specie'] == 'No audio']\n",
    "\n",
    "# If there are basename of predictions that match basename of no_audio_annotations, they are false positive\n",
    "if not no_audio_annotations.empty:\n",
    "    for _, row in no_audio_annotations.iterrows():\n",
    "        if row['basename'] in predictions_df['basename'].values:\n",
    "            FP += len(predictions_df[predictions_df['basename'] == row['basename']])\n",
    "            # Append FP predictions\n",
    "            for _, prediction in predictions_df[predictions_df['basename'] == row['basename']].iterrows():\n",
    "                filename = str(prediction['basename'] + \"_\" + str(prediction[\"start_time\"]) + \"_\" + str(prediction[\"end_time\"]))\n",
    "                fp_predictions.append({'file': filename, 'reason': 'Predicted audio in no-audio segment'})\n",
    "        else:\n",
    "            TN += len(predictions_df[predictions_df['basename'] == row['basename']])\n",
    "\n",
    "# Eliminate empty audios from annotations_df\n",
    "annotations_df = annotations_df[annotations_df['specie'] != 'No audio']\n",
    "\n",
    "# Loop through all predictions\n",
    "for _, prediction in predictions_df.iterrows():\n",
    "    matched = False\n",
    "    for _, row in annotations_df.iterrows():\n",
    "        annotation_interval = [row['start_time'], row['end_time']]\n",
    "        prediction_interval = [prediction['start_time'], prediction['end_time']]\n",
    "        iou = calculate_iou(prediction_interval, annotation_interval)\n",
    "        if iou >= iou_threshold and prediction['basename'] == row['basename']:\n",
    "            TP += 1\n",
    "            matched = True\n",
    "            break\n",
    "    if not matched:\n",
    "        FN += 1\n",
    "        filename = str(prediction['basename'] + \"_\" + str(prediction[\"start_time\"]) + \"_\" + str(prediction[\"end_time\"]))\n",
    "        fn_predictions.append({'file': filename, 'reason': 'No matching annotation found'})\n",
    "    \n",
    "# Convert lists to DataFrames\n",
    "fp_df = pd.DataFrame(fp_predictions)\n",
    "fn_df = pd.DataFrame(fn_predictions)\n",
    "\n",
    "# Calcular métricas de rendimiento\n",
    "accuracy = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN > 0 else 0\n",
    "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "print(f\"TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRDeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
