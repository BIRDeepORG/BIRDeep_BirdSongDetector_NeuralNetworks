{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Ground Truths:  542\n"
     ]
    }
   ],
   "source": [
    "# Ground Truth CSV\n",
    "PATH = \"../Data/\"\n",
    "gt_csv = PATH + \"Dataset/CSVs/test_with_bg.csv\"\n",
    "\n",
    "# Read the Ground Truth CSV\n",
    "gt_df = pd.read_csv(gt_csv)\n",
    "print(\"Número de Ground Truths: \", len(gt_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to group predictions and ground truth annotations (if same specie and overlapping times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_consecutive_predictions_birdnet(predictions_df):\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    for _, row in predictions_df.iterrows():\n",
    "        if current_group and (row['Start (s)'] != last_end or row['Scientific name'] != current_group[-1]['Scientific name']):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['End (s)']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "    # Combine groups in unique predictions\n",
    "    combined_predictions = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'Start (s)': group[0]['Start (s)'],\n",
    "            'End (s)': group[-1]['End (s)'],\n",
    "            'Scientific name': group[0]['Scientific name'],\n",
    "            'Confidence': max(item['Confidence'] for item in group)  # conf = max confidence in group\n",
    "        }\n",
    "        combined_predictions.append(combined_prediction)\n",
    "    return combined_predictions\n",
    "\n",
    "def group_consecutive_annotations_gt(annotations_df):\n",
    "    annotations_df.sort_values(by=['path', 'specie', 'start_time'], inplace=True)\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    last_path = None\n",
    "    last_specie = None\n",
    "\n",
    "    for _, row in annotations_df.iterrows():\n",
    "        if current_group and (row['path'] != last_path or row['specie'] != last_specie or row['start_time'] - last_end >= 1):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['end_time']\n",
    "        last_path = row['path']\n",
    "        last_specie = row['specie']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "\n",
    "    # Combine groups into unique predictions\n",
    "    combined_annotations = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'path': group[0]['path'],\n",
    "            'start_time': group[0]['start_time'],\n",
    "            'end_time': group[-1]['end_time'],\n",
    "            'specie': group[0]['specie'],\n",
    "            # Assuming bbox or confidence needs to be handled here. Adjust as necessary.\n",
    "            # 'Confidence': max(item['Confidence'] for item in group)  # Example for confidence\n",
    "        }\n",
    "        combined_annotations.append(combined_prediction)\n",
    "    \n",
    "    return combined_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Ground Truths agrupados:  469\n"
     ]
    }
   ],
   "source": [
    "gt_df = group_consecutive_annotations_gt(gt_df)\n",
    "gt_df = pd.DataFrame(gt_df)  # Convert list dict to DataFrame\n",
    "print(\"Número de Ground Truths agrupados: \", len(gt_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the IoU\n",
    "def calculate_iou(interval1, interval2):\n",
    "    start_max = max(interval1[0], interval2[0])\n",
    "    end_min = min(interval1[1], interval2[1])\n",
    "    intersection = max(0, end_min - start_max)\n",
    "    union = (interval1[1] - interval1[0]) + (interval2[1] - interval2[0]) - intersection\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Function to check if a prediction is correct\\ndef is_prediction_correct_detector_birdnet(prediction, gt_annotation, iou_threshold):\\n    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\\n    return iou >= iou_threshold\\n\\n# Function to check if a prediction is correct\\ndef is_prediction_correct_birdnet(prediction, gt_annotation, iou_threshold):\\n    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\\n    prediction_class = prediction['Scientific name'].lower()\\n    gt_class = gt_annotation['specie'].lower()\\n    scientific_name_matches = prediction_class == gt_class\\n    return iou >= iou_threshold and scientific_name_matches\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_detection_birdnet(gt_row, grouped_predictions, confidence_threshold, iou_threshold=0):\n",
    "    for prediction in grouped_predictions:\n",
    "        iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_row['start_time'], gt_row['end_time']))\n",
    "        if iou >= iou_threshold and prediction['Confidence'] >= confidence_threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_detection_classification_birdnet(gt_row, grouped_predictions, confidence_threshold, iou_threshold=0):\n",
    "    for prediction in grouped_predictions:\n",
    "        iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_row['start_time'], gt_row['end_time']))\n",
    "        if (prediction['Scientific name'].lower() == gt_row['specie'].lower() and\n",
    "            iou >= iou_threshold and\n",
    "            prediction['Confidence'] >= confidence_threshold):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "'''def is_detection_birdnet(gt_row, grouped_predictions, confidence_threshold, iou_threshold=0): \n",
    "    for prediction in grouped_predictions: \n",
    "        if not (prediction['End (s)'] < gt_row['start_time'] or prediction['Start (s)'] > gt_row['end_time']) and prediction['Confidence'] >= confidence_threshold: \n",
    "            return True \n",
    "        return False\n",
    "\n",
    "def is_detection_classification_birdnet(gt_row, grouped_predictions, confidence_threshold, iou_threshold=0): \n",
    "    for prediction in grouped_predictions: \n",
    "        if (prediction['Scientific name'].lower() == gt_row['specie'].lower() and not (prediction['End (s)'] < gt_row['start_time'] or prediction['Start (s)'] > gt_row['end_time']) and prediction['Confidence'] >= confidence_threshold): \n",
    "            return True \n",
    "    return False'''\n",
    "\n",
    "'''# Function to check if a prediction is correct\n",
    "def is_prediction_correct_detector_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    return iou >= iou_threshold\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    prediction_class = prediction['Scientific name'].lower()\n",
    "    gt_class = gt_annotation['specie'].lower()\n",
    "    scientific_name_matches = prediction_class == gt_class\n",
    "    return iou >= iou_threshold and scientific_name_matches'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions_BirdNET(analysis_name, prediction_conf_score=0.6, iou_threshold=0.1):\n",
    "    # Variables for metrics\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    total_predictions_score = 0\n",
    "\n",
    "    total_gt = len(gt_df)\n",
    "\n",
    "    # Metrics\n",
    "    true_positives_detector = 0\n",
    "    true_positives_classifier = 0\n",
    "    false_positives = 0\n",
    "    false_negatives_detector = 0\n",
    "    false_negatives_classifier = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    # Process the predictions\n",
    "    for _, gt_annotation in gt_df.iterrows():\n",
    "        # Load the predictions\n",
    "        prediction_path = f\"../BirdNET/Predictions/{analysis_name}/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "\n",
    "        try:\n",
    "            predictions_df = pd.read_csv(prediction_path)\n",
    "            grouped_predictions = group_consecutive_predictions_birdnet(predictions_df)  # Group predictions\n",
    "            predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "            total_predictions += len(grouped_predictions)\n",
    "            total_predictions_score += len([p for p in grouped_predictions if p['Confidence'] >= prediction_conf_score])\n",
    "\n",
    "            # If the annotation if not a background\n",
    "            if gt_annotation['specie'] != 'No audio':\n",
    "                # Check if the GT is detected by the detector\n",
    "                if is_detection_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    true_positives_detector += 1\n",
    "                else:\n",
    "                    false_negatives_detector += 1\n",
    "                \n",
    "                if is_detection_classification_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                    true_positives_classifier += 1\n",
    "                else:\n",
    "                    false_negatives_classifier += 1\n",
    "            else: # Annotation is a background\n",
    "                # Take only grouped_predictions with confidence >= prediction_conf_score\n",
    "                grouped_predictions = [p for p in grouped_predictions if p['Confidence'] >= prediction_conf_score]\n",
    "                if len(grouped_predictions) == 0:\n",
    "                    true_negatives += 1\n",
    "                else: # Background is unique file, with no annotations, so all predictions on that file are false positives\n",
    "                    false_positives += len(grouped_predictions)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    print(\"================== Metrics ==================\\n\")\n",
    "    print(f\"Total Predictions: {total_predictions}\")\n",
    "    print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "    print(f\"Total GT: {total_gt}\")\n",
    "    print(f\"Correct Predictions Detector: {true_positives_detector}\")\n",
    "    print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "    # Additional calculations for precision, recall, and F1-score\n",
    "    print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    accuracy_detector = (true_positives_detector + true_negatives) / (true_positives_detector + true_negatives + false_positives + false_negatives_detector) if true_positives_detector + true_negatives + false_positives + false_negatives_detector != 0 else 0\n",
    "    precision_detector = true_positives_detector / (true_positives_detector + false_positives) if true_positives_detector + false_positives != 0 else 0\n",
    "    recall_detector = true_positives_detector / (true_positives_detector + false_negatives_detector) if true_positives_detector + false_negatives_detector != 0 else 0\n",
    "    f1_score_detector = 2 * precision_detector * recall_detector / (precision_detector + recall_detector) if precision_detector + recall_detector != 0 else 0\n",
    "    print(f\"Accuracy: {accuracy_detector}\")\n",
    "    print(f\"Precision: {precision_detector}\")\n",
    "    print(f\"Recall: {recall_detector}\")\n",
    "    print(f\"F1-Score: {f1_score_detector}\")\n",
    "\n",
    "    print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    accuracy_classifier = (true_positives_classifier + true_negatives) / (true_positives_classifier + true_negatives + false_positives + false_negatives_detector) if true_positives_classifier + true_negatives + false_positives + false_negatives_classifier != 0 else 0\n",
    "    precision_classifier = true_positives_classifier / (true_positives_classifier + false_positives) if true_positives_classifier + false_positives != 0 else 0\n",
    "    recall_classifier = true_positives_classifier / (true_positives_classifier + false_negatives_detector) if true_positives_classifier + false_negatives_detector != 0 else 0\n",
    "    f1_score_classifier = 2 * precision_classifier * recall_classifier / (precision_classifier + recall_classifier) if precision_classifier + recall_classifier != 0 else 0\n",
    "    print(f\"Accuracy: {accuracy_classifier}\")\n",
    "    print(f\"Precision: {precision_classifier}\")\n",
    "    print(f\"Recall: {recall_classifier}\")\n",
    "    print(f\"F1-Score: {f1_score_classifier}\")\n",
    "\n",
    "    print(\"\\n================== Other ==================\\n\")\n",
    "    print(f\"False Positives: {false_positives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Base con la lista de especies de Doñana de BIRDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 2341\n",
      "Total Predictions with Score >= 0.6: 527\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 60\n",
      "Correct Predictions Detector + Classifier: 13\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.17270788912579957\n",
      "Precision: 1.0\n",
      "Recall: 0.13392857142857142\n",
      "F1-Score: 0.23622047244094488\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.08056872037914692\n",
      "Precision: 1.0\n",
      "Recall: 0.032418952618453865\n",
      "F1-Score: 0.06280193236714976\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 0\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"0_BirdNet_Base_AllTest_DonanaSpecies\", 0.6, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Base con la lista de especies del customClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 2798\n",
      "Total Predictions with Score >= 0.6: 751\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 79\n",
      "Correct Predictions Detector + Classifier: 26\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.21108742004264391\n",
      "Precision: 0.9875\n",
      "Recall: 0.17633928571428573\n",
      "F1-Score: 0.29924242424242425\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.11057692307692307\n",
      "Precision: 0.9629629629629629\n",
      "Recall: 0.06582278481012659\n",
      "F1-Score: 0.12322274881516589\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 1\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"1_BirdNet_Base_AllTest_ClassifierSpecies\", 0.6, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 2798\n",
      "Total Predictions with Score >= 0.1: 2798\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 175\n",
      "Correct Predictions Detector + Classifier: 66\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.4088983050847458\n",
      "Precision: 0.9668508287292817\n",
      "Recall: 0.390625\n",
      "F1-Score: 0.5564387917329094\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.23140495867768596\n",
      "Precision: 0.9166666666666666\n",
      "Recall: 0.19469026548672566\n",
      "F1-Score: 0.32116788321167883\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 6\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"1_BirdNet_Base_AllTest_ClassifierSpecies\", 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer con las predicciones de yolo como detectorr\n",
    "\n",
    "# Correr inference.py desde la VPN, intentar encontrar mejores valores de iou y conf y poner esos para las métricas aquí, luego coger esos valores y ponerlos\n",
    "\n",
    "# predictionms de val de yolov8:\n",
    "'''\n",
    "[{\"image_id\": \"AM4_20230531_110000\", \"category_id\": 0, \"bbox\": [301.605, 0.0, 31.041, 459.45], \"score\": 0.5276}, {\"image_id\": \"AM15_20230712_074000\", \"category_id\": 0, \"bbox\": [61.011, 0.439, 175.454, 460.228], \"score\": 0.48407}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [57.914, 0.573, 30.844, 461.131], \"score\": 0.65271}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [231.294, 1.093, 31.147, 460.902], \"score\": 0.53149}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [620.696, 2.401, 34.881, 459.599], \"score\": 0.44922}, {\"image_id\": \"AM8_20230304_093000\", \"category_id\": 0, \"bbox\": [765.516, 0.0, 162.609, 460.58], \"score\": 0.63678}, \n",
    "'''\n",
    "\n",
    "# Y si se hace predict sobre la carpeta test??\n",
    "\n",
    "\n",
    "# Copilot:\n",
    "'''\n",
    "Ahoar quiero modificar estas funciones:\n",
    "\n",
    "def group_consecutive_predictions_birdnet(predictions_df):\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    for _, row in predictions_df.iterrows():\n",
    "        if current_group and (row['Start (s)'] != last_end or row['Scientific name'] != current_group[-1]['Scientific name']):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['End (s)']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "    # Combine groups in unique predictions\n",
    "    combined_predictions = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'Start (s)': group[0]['Start (s)'],\n",
    "            'End (s)': group[-1]['End (s)'],\n",
    "            'Scientific name': group[0]['Scientific name'],\n",
    "            'Confidence': max(item['Confidence'] for item in group)  # conf = max confidence in group\n",
    "        }\n",
    "        combined_predictions.append(combined_prediction)\n",
    "    return combined_predictions\n",
    "\n",
    "# Function to calculate the IoU\n",
    "def calculate_iou(interval1, interval2):\n",
    "    start_max = max(interval1[0], interval2[0])\n",
    "    end_min = min(interval1[1], interval2[1])\n",
    "    intersection = max(0, end_min - start_max)\n",
    "    union = (interval1[1] - interval1[0]) + (interval2[1] - interval2[0]) - intersection\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct_detector(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    return iou >= iou_threshold\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    prediction_class = prediction['Scientific name'].lower()\n",
    "    gt_class = gt_annotation['specie'].lower()\n",
    "    scientific_name_matches = prediction_class == gt_class\n",
    "    return iou >= iou_threshold and scientific_name_matches\n",
    "\n",
    "    Y este codigo:\n",
    "\n",
    "    # Variables for metrics\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "total_predictions_score = 0\n",
    "correct_predictions_detector = 0\n",
    "iou_threshold = 0.4  # This value is editable\n",
    "prediction_conf_score = 0.4  # This value is editable\n",
    "\n",
    "# Process the predictions\n",
    "for _, gt_annotation in gt_df.iterrows():\n",
    "    prediction_path = PATH + f\"Dataset/BirdNET_Predictions/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "    try:\n",
    "        predictions_df = pd.read_csv(prediction_path)\n",
    "        grouped_predictions = group_consecutive_predictions_birdnet(predictions_df)  # Group predictions\n",
    "        predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "        for _, prediction in predictions_df.iterrows():\n",
    "            total_predictions += 1\n",
    "            if prediction['Confidence'] >= prediction_conf_score:\n",
    "                total_predictions_score += 1\n",
    "                if is_prediction_correct(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                if is_prediction_correct_detector(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions_detector += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "# Calculate and display the metrics\n",
    "print(\"================== Metrics ==================\\n\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "print(f\"Total GT: {len(gt_df)}\")\n",
    "print(f\"Correct Predictions Detector: {correct_predictions_detector}\")\n",
    "print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "# Additional calculations for precision, recall, and F1-score\n",
    "print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "true_positives = correct_predictions_detector\n",
    "false_positives = total_predictions_score - correct_predictions_detector\n",
    "false_negatives = len(gt_df) - correct_predictions_detector\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "true_positives = correct_predictions\n",
    "false_positives = total_predictions_score - correct_predictions\n",
    "false_negatives = len(gt_df) - correct_predictions\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "Para que funcione sobre el fichero predictions.json con la estructura de datos:\n",
    "[{\"image_id\": \"AM4_20230531_110000\", \"category_id\": 0, \"bbox\": [301.605, 0.0, 31.041, 459.45], \"score\": 0.5276}, {\"image_id\": \"AM15_20230712_074000\", \"category_id\": 0, \"bbox\": [61.011, 0.439, 175.454, 460.228], \"score\": 0.48407}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [57.914, 0.573, 30.844, 461.131], \"score\": 0.65271}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [231.294, 1.093, 31.147, 460.902], \"score\": 0.53149}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [620.696, 2.401, 34.881, 459.599], \"score\": 0.44922}, {\"image_id\": \"AM8_20230304_093000\", \"category_id\": 0, \"bbox\": [765.516, 0.0, 162.609, 460.58], \"score\": 0.63678}, ...\n",
    "\n",
    "En la que \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Preentrenado con la lista de especies de Doñana de BIRDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 7559\n",
      "Total Predictions with Score >= 0.6: 167\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 17\n",
      "Correct Predictions Detector + Classifier: 6\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.08102345415778252\n",
      "Precision: 1.0\n",
      "Recall: 0.03794642857142857\n",
      "F1-Score: 0.07311827956989246\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.05895196506550218\n",
      "Precision: 1.0\n",
      "Recall: 0.013729977116704805\n",
      "F1-Score: 0.02708803611738149\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 0\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"2_BirdNet_FineTuning_AllTest\", 0.6, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 7559\n",
      "Total Predictions with Score >= 0.1: 7559\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 280\n",
      "Correct Predictions Detector + Classifier: 104\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.5868263473053892\n",
      "Precision: 0.877742946708464\n",
      "Recall: 0.625\n",
      "F1-Score: 0.7301173402868317\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.3630769230769231\n",
      "Precision: 0.7272727272727273\n",
      "Recall: 0.38235294117647056\n",
      "F1-Score: 0.5012048192771085\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 39\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"2_BirdNet_FineTuning_AllTest\", 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET customClassifier con los recortes de YOLOv8 para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 2652\n",
      "Total Predictions with Score >= 0.6: 106\n",
      "Total GT: 542\n",
      "Correct Predictions Detector: 15\n",
      "Correct Predictions Detector + Classifier: 3\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.023696682464454975\n",
      "Precision: 0.14150943396226415\n",
      "Recall: 0.027675276752767528\n",
      "F1-Score: 0.0462962962962963\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.004651162790697674\n",
      "Precision: 0.02830188679245283\n",
      "Recall: 0.005535055350553505\n",
      "F1-Score: 0.009259259259259257\n"
     ]
    }
   ],
   "source": [
    "# Hacer con las predicciones de BIRDNET despues de entrenar sobre los recortes del detector de yolov8\n",
    "\n",
    "# Hacer con las predicciones de BirdNet después de haber entrenado sobre el conjunto de test total\n",
    "# Variables for metrics\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "total_predictions_score = 0\n",
    "correct_predictions_detector = 0\n",
    "iou_threshold = 0.1  # This value is editable\n",
    "prediction_conf_score = 0.6  # This value is editable\n",
    "\n",
    "# Process the predictions\n",
    "for _, gt_annotation in gt_df.iterrows():\n",
    "    gt_basename = gt_annotation['path'].split('/')[-1]\n",
    "    # Remove the extension and potential suffix _X\n",
    "    gt_basename_no_ext = os.path.splitext(gt_basename)[0]\n",
    "    if \"_\" in gt_basename_no_ext:\n",
    "        gt_basename_no_ext = \"_\".join(gt_basename_no_ext.split(\"_\")[:-1])\n",
    "    prediction_pattern = f\"../BirdNET/Predictions/3_BirdNET_FineTuning_DetectorTest/{gt_basename_no_ext}_*.BirdNET.results.csv\"\n",
    "    \n",
    "    # Find all matching prediction files\n",
    "    prediction_files = glob.glob(prediction_pattern)\n",
    "    for prediction_path in prediction_files:\n",
    "        try:\n",
    "            predictions_df = pd.read_csv(prediction_path)\n",
    "            grouped_predictions = group_consecutive_predictions_birdnet(predictions_df)  # Group predictions\n",
    "            predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "            for _, prediction in predictions_df.iterrows():\n",
    "                total_predictions += 1\n",
    "                if prediction['Confidence'] >= prediction_conf_score:\n",
    "                    total_predictions_score += 1\n",
    "                    if is_prediction_correct_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "                        correct_predictions += 1\n",
    "                    if is_prediction_correct_detector_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "                        correct_predictions_detector += 1\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Prediction file not found: {prediction_path}\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"No data in file: {prediction_path}\")\n",
    "\n",
    "# Calculate and display the metrics\n",
    "print(\"================== Metrics ==================\\n\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "print(f\"Total GT: {len(gt_df)}\")\n",
    "print(f\"Correct Predictions Detector: {correct_predictions_detector}\")\n",
    "print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "# Additional calculations for precision, recall, and F1-score\n",
    "print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "true_positives = correct_predictions_detector\n",
    "false_positives = total_predictions_score - correct_predictions_detector\n",
    "false_negatives = len(gt_df) - correct_predictions_detector\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "true_positives = correct_predictions\n",
    "false_positives = total_predictions_score - correct_predictions\n",
    "false_negatives = len(gt_df) - correct_predictions\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "'''\n",
    "Quiero editar esta función para que funcione con predicciones que están estructuradas diferente. Esta es la función:\n",
    "\n",
    "def analyze_predictions_BirdNET(analysis_name, prediction_conf_score=0.6, iou_threshold=0.1):\n",
    "    # Variables for metrics\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    total_predictions_score = 0\n",
    "\n",
    "    total_gt = len(gt_df)\n",
    "\n",
    "    # Metrics\n",
    "    true_positives_detector = 0\n",
    "    true_positives_classifier = 0\n",
    "    false_positives = 0\n",
    "    false_negatives_detector = 0\n",
    "    false_negatives_classifier = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    # Process the predictions\n",
    "    for _, gt_annotation in gt_df.iterrows():\n",
    "        # Load the predictions\n",
    "        prediction_path = f\"../BirdNET/Predictions/{analysis_name}/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "\n",
    "        try:\n",
    "            predictions_df = pd.read_csv(prediction_path)\n",
    "            grouped_predictions = group_consecutive_predictions_birdnet(predictions_df)  # Group predictions\n",
    "            predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "            total_predictions += len(grouped_predictions)\n",
    "            total_predictions_score += len([p for p in grouped_predictions if p['Confidence'] >= prediction_conf_score])\n",
    "\n",
    "            # If the annotation if not a background\n",
    "            if gt_annotation['specie'] != 'No audio':\n",
    "                # Check if the GT is detected by the detector\n",
    "                if is_detection_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    true_positives_detector += 1\n",
    "                else:\n",
    "                    false_negatives_detector += 1\n",
    "                \n",
    "                if is_detection_classification_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                    true_positives_classifier += 1\n",
    "                else:\n",
    "                    false_negatives_classifier += 1\n",
    "            else: # Annotation is a background\n",
    "                # Take only grouped_predictions with confidence >= prediction_conf_score\n",
    "                grouped_predictions = [p for p in grouped_predictions if p['Confidence'] >= prediction_conf_score]\n",
    "                if len(grouped_predictions) == 0:\n",
    "                    true_negatives += 1\n",
    "                else: # Background is unique file, with no annotations, so all predictions on that file are false positives\n",
    "                    false_positives += len(grouped_predictions)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    print(\"================== Metrics ==================\\n\")\n",
    "    print(f\"Total Predictions: {total_predictions}\")\n",
    "    print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "    print(f\"Total GT: {total_gt}\")\n",
    "    print(f\"Correct Predictions Detector: {true_positives_detector}\")\n",
    "    print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "    # Additional calculations for precision, recall, and F1-score\n",
    "    print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    accuracy_detector = (true_positives_detector + true_negatives) / (true_positives_detector + true_negatives + false_positives + false_negatives_detector) if true_positives_detector + true_negatives + false_positives + false_negatives_detector != 0 else 0\n",
    "    precision_detector = true_positives_detector / (true_positives_detector + false_positives) if true_positives_detector + false_positives != 0 else 0\n",
    "    recall_detector = true_positives_detector / (true_positives_detector + false_negatives_detector) if true_positives_detector + false_negatives_detector != 0 else 0\n",
    "    f1_score_detector = 2 * precision_detector * recall_detector / (precision_detector + recall_detector) if precision_detector + recall_detector != 0 else 0\n",
    "    print(f\"Accuracy: {accuracy_detector}\")\n",
    "    print(f\"Precision: {precision_detector}\")\n",
    "    print(f\"Recall: {recall_detector}\")\n",
    "    print(f\"F1-Score: {f1_score_detector}\")\n",
    "\n",
    "    print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    accuracy_classifier = (true_positives_classifier + true_negatives) / (true_positives_classifier + true_negatives + false_positives + false_negatives_detector) if true_positives_classifier + true_negatives + false_positives + false_negatives_classifier != 0 else 0\n",
    "    precision_classifier = true_positives_classifier / (true_positives_classifier + false_positives) if true_positives_classifier + false_positives != 0 else 0\n",
    "    recall_classifier = true_positives_classifier / (true_positives_classifier + false_negatives_detector) if true_positives_classifier + false_negatives_detector != 0 else 0\n",
    "    f1_score_classifier = 2 * precision_classifier * recall_classifier / (precision_classifier + recall_classifier) if precision_classifier + recall_classifier != 0 else 0\n",
    "    print(f\"Accuracy: {accuracy_classifier}\")\n",
    "    print(f\"Precision: {precision_classifier}\")\n",
    "    print(f\"Recall: {recall_classifier}\")\n",
    "    print(f\"F1-Score: {f1_score_classifier}\")\n",
    "\n",
    "    print(\"\\n================== Other ==================\\n\")\n",
    "    print(f\"False Positives: {false_positives}\")\n",
    "\n",
    "\n",
    "    Y la estructura de las predicciones es, dentro de la carpeta analysis_name estan directamente los ficheros con el nombre gt_basename_no_ext:\n",
    "\n",
    "    for _, gt_annotation in gt_df.iterrows():\n",
    "    gt_basename = gt_annotation['path'].split('/')[-1]\n",
    "    # Remove the extension and potential suffix _X\n",
    "    gt_basename_no_ext = os.path.splitext(gt_basename)[0]\n",
    "\n",
    "Pero hay varios ficheros para un mismo audios\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRDeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
