{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Ground Truths:  542\n"
     ]
    }
   ],
   "source": [
    "# Ground Truth CSV\n",
    "PATH = \"../Data/\"\n",
    "gt_csv = PATH + \"Dataset/CSVs/test_with_bg.csv\"\n",
    "\n",
    "# Read the Ground Truth CSV\n",
    "gt_df = pd.read_csv(gt_csv)\n",
    "print(\"Número de Ground Truths: \", len(gt_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to group predictions and ground truth annotations (if same specie and overlapping times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_consecutive_predictions_birdnet(predictions_df):\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    for _, row in predictions_df.iterrows():\n",
    "        if current_group and (row['Start (s)'] != last_end or row['Scientific name'] != current_group[-1]['Scientific name']):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['End (s)']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "    # Combine groups in unique predictions\n",
    "    combined_predictions = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'Start (s)': group[0]['Start (s)'],\n",
    "            'End (s)': group[-1]['End (s)'],\n",
    "            'Scientific name': group[0]['Scientific name'],\n",
    "            'Confidence': max(item['Confidence'] for item in group)  # conf = max confidence in group\n",
    "        }\n",
    "        combined_predictions.append(combined_prediction)\n",
    "    return combined_predictions\n",
    "\n",
    "def group_consecutive_annotations_gt(annotations_df):\n",
    "    annotations_df.sort_values(by=['path', 'specie', 'start_time'], inplace=True)\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    last_path = None\n",
    "    last_specie = None\n",
    "\n",
    "    for _, row in annotations_df.iterrows():\n",
    "        if current_group and (row['path'] != last_path or row['specie'] != last_specie or row['start_time'] - last_end >= 1):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['end_time']\n",
    "        last_path = row['path']\n",
    "        last_specie = row['specie']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "\n",
    "    # Combine groups into unique predictions\n",
    "    combined_annotations = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'path': group[0]['path'],\n",
    "            'start_time': group[0]['start_time'],\n",
    "            'end_time': group[-1]['end_time'],\n",
    "            'specie': group[0]['specie'],\n",
    "            # Assuming bbox or confidence needs to be handled here. Adjust as necessary.\n",
    "            # 'Confidence': max(item['Confidence'] for item in group)  # Example for confidence\n",
    "        }\n",
    "        combined_annotations.append(combined_prediction)\n",
    "    \n",
    "    return combined_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Ground Truths agrupados:  469\n"
     ]
    }
   ],
   "source": [
    "gt_df = group_consecutive_annotations_gt(gt_df)\n",
    "gt_df = pd.DataFrame(gt_df)  # Convert list dict to DataFrame\n",
    "print(\"Número de Ground Truths agrupados: \", len(gt_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the IoU\n",
    "def calculate_iou(interval1, interval2):\n",
    "    start_max = max(interval1[0], interval2[0])\n",
    "    end_min = min(interval1[1], interval2[1])\n",
    "    intersection = max(0, end_min - start_max)\n",
    "    union = (interval1[1] - interval1[0]) + (interval2[1] - interval2[0]) - intersection\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_detection_birdnet(gt_row, grouped_predictions, confidence_threshold, iou_threshold=0):\n",
    "    for prediction in grouped_predictions:\n",
    "        if prediction['Confidence'] >= confidence_threshold:\n",
    "            iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_row['start_time'], gt_row['end_time']))\n",
    "            if iou >= iou_threshold:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def is_detection_classification_birdnet(gt_row, grouped_predictions, confidence_threshold, iou_threshold=0):\n",
    "    for prediction in grouped_predictions:\n",
    "        if prediction['Confidence'] >= confidence_threshold:\n",
    "            iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_row['start_time'], gt_row['end_time']))\n",
    "            if (prediction['Scientific name'].lower() == gt_row['specie'].lower() and\n",
    "                iou >= iou_threshold):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(total_predictions, prediction_conf_score, total_predictions_score, total_gt, true_positives_detector, true_positives_classifier, correct_predictions, false_positives, false_negatives_detector, false_negatives_classifier, true_negatives):\n",
    "    print(\"================== Metrics ==================\\n\")\n",
    "    print(f\"Total Predictions: {total_predictions}\")\n",
    "    print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "    print(f\"Total GT: {total_gt}\")\n",
    "    print(f\"Correct Predictions Detector: {true_positives_detector}\")\n",
    "    print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "    # Additional calculations for precision, recall, and F1-score\n",
    "    print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    accuracy_detector = (true_positives_detector + true_negatives) / (true_positives_detector + true_negatives + false_positives + false_negatives_detector) if true_positives_detector + true_negatives + false_positives + false_negatives_detector != 0 else 0\n",
    "    precision_detector = true_positives_detector / (true_positives_detector + false_positives) if true_positives_detector + false_positives != 0 else 0\n",
    "    recall_detector = true_positives_detector / (true_positives_detector + false_negatives_detector) if true_positives_detector + false_negatives_detector != 0 else 0\n",
    "    f1_score_detector = 2 * precision_detector * recall_detector / (precision_detector + recall_detector) if precision_detector + recall_detector != 0 else 0\n",
    "    print(f\"Accuracy: {accuracy_detector}\")\n",
    "    print(f\"Precision: {precision_detector}\")\n",
    "    print(f\"Recall: {recall_detector}\")\n",
    "    print(f\"F1-Score: {f1_score_detector}\")\n",
    "\n",
    "    print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    accuracy_classifier = (true_positives_classifier + true_negatives) / (true_positives_classifier + true_negatives + false_positives + false_negatives_detector) if true_positives_classifier + true_negatives + false_positives + false_negatives_classifier != 0 else 0\n",
    "    precision_classifier = true_positives_classifier / (true_positives_classifier + false_positives) if true_positives_classifier + false_positives != 0 else 0\n",
    "    recall_classifier = true_positives_classifier / (true_positives_classifier + false_negatives_detector) if true_positives_classifier + false_negatives_detector != 0 else 0\n",
    "    f1_score_classifier = 2 * precision_classifier * recall_classifier / (precision_classifier + recall_classifier) if precision_classifier + recall_classifier != 0 else 0\n",
    "    print(f\"Accuracy: {accuracy_classifier}\")\n",
    "    print(f\"Precision: {precision_classifier}\")\n",
    "    print(f\"Recall: {recall_classifier}\")\n",
    "    print(f\"F1-Score: {f1_score_classifier}\")\n",
    "\n",
    "    print(\"\\n================== Other ==================\\n\")\n",
    "    print(f\"False Positives: {false_positives}\")\n",
    "\n",
    "def analyze_predictions_BirdNET(analysis_name, prediction_conf_score=0.6, iou_threshold=0.1):\n",
    "    # Variables for metrics\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    total_predictions_score = 0\n",
    "\n",
    "    total_gt = len(gt_df)\n",
    "\n",
    "    # Metrics\n",
    "    true_positives_detector = 0\n",
    "    true_positives_classifier = 0\n",
    "    false_positives = 0\n",
    "    false_negatives_detector = 0\n",
    "    false_negatives_classifier = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    last_file = None\n",
    "\n",
    "    # Process the predictions\n",
    "    for _, gt_annotation in gt_df.iterrows():\n",
    "        # Load the predictions\n",
    "        prediction_path = f\"../BirdNET/Predictions/{analysis_name}/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "\n",
    "        current_file = gt_annotation['path']\n",
    "\n",
    "        try:\n",
    "            predictions_df = pd.read_csv(prediction_path)\n",
    "            grouped_predictions = group_consecutive_predictions_birdnet(predictions_df)  # Group predictions\n",
    "\n",
    "            if current_file != last_file:\n",
    "                total_predictions += len(grouped_predictions)\n",
    "                current_predictions_score = len([p for p in grouped_predictions if p['Confidence'] >= prediction_conf_score])\n",
    "                total_predictions_score += current_predictions_score\n",
    "\n",
    "            # If the annotation if not a background\n",
    "            if gt_annotation['specie'] != 'No audio':\n",
    "                # Check if the GT is detected by the detector\n",
    "                if is_detection_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    true_positives_detector += 1\n",
    "                else:\n",
    "                    false_negatives_detector += 1\n",
    "                \n",
    "                if is_detection_classification_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                    true_positives_classifier += 1\n",
    "                else:\n",
    "                    false_negatives_classifier += 1\n",
    "            else: # Annotation is a background\n",
    "                # Take only grouped_predictions with confidence >= prediction_conf_score\n",
    "                if current_predictions_score == 0:\n",
    "                    true_negatives += 1\n",
    "                else: # Background is unique file, with no annotations, so all predictions on that file are false positives\n",
    "                    false_positives += current_predictions_score\n",
    "\n",
    "            last_file = current_file\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    display_metrics(total_predictions, prediction_conf_score, total_predictions_score, total_gt, true_positives_detector, true_positives_classifier, correct_predictions, false_positives, false_negatives_detector, false_negatives_classifier, true_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions_YOLO_BirdNET(analysis_name, prediction_conf_score=0.6, iou_threshold=0.1):\n",
    "    # Variables for metrics\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    total_predictions_score = 0\n",
    "    total_gt = len(gt_df)\n",
    "    # Metrics\n",
    "    true_positives_detector = 0\n",
    "    true_positives_classifier = 0\n",
    "    false_positives = 0\n",
    "    false_negatives_detector = 0\n",
    "    false_negatives_classifier = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    last_file = None\n",
    "\n",
    "    # Process the predictions\n",
    "    for _, gt_annotation in gt_df.iterrows():\n",
    "        gt_basename = gt_annotation['path'].split('/')[-1]\n",
    "        gt_basename_no_ext = os.path.splitext(gt_basename)[0]\n",
    "        # Assuming the structure of the file name is consistent with the example given\n",
    "        prediction_files = os.listdir(f\"../BirdNET/Predictions/{analysis_name}\")\n",
    "        prediction_files_filtered = [file for file in prediction_files if file.startswith(gt_basename_no_ext)]\n",
    "\n",
    "        current_file = gt_annotation['path']\n",
    "        \n",
    "        # predictions_df initialization\n",
    "        predictions_df = pd.DataFrame(columns=['Start (s)', 'End (s)', 'Scientific name', 'Common name', 'Confidence'])\n",
    "\n",
    "        for prediction_file in prediction_files_filtered:\n",
    "            start_time, end_time = prediction_file.split('_')[-2:]\n",
    "            end_time = end_time.split('.BirdNET')[0]\n",
    "            start_time = float(start_time)\n",
    "            end_time = float(end_time)\n",
    "            prediction_path = f\"../BirdNET/Predictions/{analysis_name}/{prediction_file}\"\n",
    "\n",
    "            # Create prediction DataFrame and append all of current prediction file\n",
    "            current_prediction_df = pd.read_csv(prediction_path)\n",
    "\n",
    "            # Set start and end times\n",
    "            current_prediction_df['Start (s)'] = float(start_time)\n",
    "            current_prediction_df['End (s)'] = float(end_time)\n",
    "\n",
    "            # If current_prediction_df is not empty, concatenate it to predictions_df\n",
    "            if not current_prediction_df.empty:\n",
    "                if predictions_df.empty:\n",
    "                    predictions_df = current_prediction_df\n",
    "                else:\n",
    "                    predictions_df = pd.concat([predictions_df, current_prediction_df], ignore_index=True)\n",
    "\n",
    "        grouped_predictions = group_consecutive_predictions_birdnet(predictions_df)  # Group predictions\n",
    "        \n",
    "        try:\n",
    "            if current_file != last_file:\n",
    "                total_predictions += len(grouped_predictions)\n",
    "                current_predictions_score = len([p for p in grouped_predictions if p['Confidence'] >= prediction_conf_score])\n",
    "                total_predictions_score += current_predictions_score\n",
    "            # If the annotation if not a background\n",
    "            if gt_annotation['specie'] != 'No audio':\n",
    "                # Check if the GT is detected by the detector\n",
    "                if is_detection_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    true_positives_detector += 1\n",
    "                else:\n",
    "                    false_negatives_detector += 1\n",
    "                \n",
    "                if is_detection_classification_birdnet(gt_annotation, grouped_predictions, prediction_conf_score, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                    true_positives_classifier += 1\n",
    "                else:\n",
    "                    false_negatives_classifier += 1\n",
    "            else: # Annotation is a background\n",
    "                # Take only grouped_predictions with confidence >= prediction_conf_score\n",
    "                if current_predictions_score == 0:\n",
    "                    true_negatives += 1\n",
    "                else: # Background is unique file, with no annotations, so all predictions on that file are false positives\n",
    "                    false_positives += current_predictions_score\n",
    "            \n",
    "            last_file = current_file\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "    # Calculate and display the metrics\n",
    "    display_metrics(total_predictions, prediction_conf_score, total_predictions_score, total_gt, true_positives_detector, true_positives_classifier, correct_predictions, false_positives, false_negatives_detector, false_negatives_classifier, true_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Base con la lista de especies de Doñana de BIRDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 194\n",
      "Total Predictions with Score >= 0.6: 27\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 35\n",
      "Correct Predictions Detector + Classifier: 9\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.11940298507462686\n",
      "Precision: 1.0\n",
      "Recall: 0.078125\n",
      "F1-Score: 0.14492753623188406\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.06772009029345373\n",
      "Precision: 1.0\n",
      "Recall: 0.02132701421800948\n",
      "F1-Score: 0.04176334106728538\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 0\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"0_BirdNet_Base_AllTest_DonanaSpecies\", 0.6, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Base con la lista de especies del customClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 160\n",
      "Total Predictions with Score >= 0.6: 39\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 48\n",
      "Correct Predictions Detector + Classifier: 19\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.14498933901918976\n",
      "Precision: 0.9795918367346939\n",
      "Recall: 0.10714285714285714\n",
      "F1-Score: 0.19315895372233396\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.08863636363636364\n",
      "Precision: 0.95\n",
      "Recall: 0.045346062052505964\n",
      "F1-Score: 0.08656036446469248\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 1\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"1_BirdNet_Base_AllTest_ClassifierSpecies\", 0.6, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 160\n",
      "Total Predictions with Score >= 0.1: 160\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 119\n",
      "Correct Predictions Detector + Classifier: 54\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.2902542372881356\n",
      "Precision: 0.952\n",
      "Recall: 0.265625\n",
      "F1-Score: 0.41535776614310643\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.1769041769041769\n",
      "Precision: 0.9\n",
      "Recall: 0.1409921671018277\n",
      "F1-Score: 0.24379232505643345\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 6\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"1_BirdNet_Base_AllTest_ClassifierSpecies\", 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Fine Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 527\n",
      "Total Predictions with Score >= 0.6: 10\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 11\n",
      "Correct Predictions Detector + Classifier: 5\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.06823027718550106\n",
      "Precision: 1.0\n",
      "Recall: 0.024553571428571428\n",
      "F1-Score: 0.04793028322440087\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.056155507559395246\n",
      "Precision: 1.0\n",
      "Recall: 0.011312217194570135\n",
      "F1-Score: 0.02237136465324385\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 0\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"2_BirdNet_FineTuning_AllTest\", 0.6, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 527\n",
      "Total Predictions with Score >= 0.1: 527\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 215\n",
      "Correct Predictions Detector + Classifier: 85\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.45708582834331335\n",
      "Precision: 0.8464566929133859\n",
      "Recall: 0.4799107142857143\n",
      "F1-Score: 0.6125356125356126\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.2668463611859838\n",
      "Precision: 0.6854838709677419\n",
      "Recall: 0.2672955974842767\n",
      "F1-Score: 0.3846153846153846\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 39\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_BirdNET(\"2_BirdNet_FineTuning_AllTest\", 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baja los scores de las clases con las que ha sido entrenado, teniendo que bajar la threshold mucho. Haciendo que haya muchos más Falsos Positivos (en los casos anteriores con 0.6 -> 0, 6... FP, ahora hay que bajar hasta 0.1 el accuracy es mejor pero tenemos muchos Falsos Positivos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET customClassifier con los recortes de YOLOv8 para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 79\n",
      "Total Predictions with Score >= 0.2: 37\n",
      "Total GT: 469\n",
      "Correct Predictions Detector: 15\n",
      "Correct Predictions Detector + Classifier: 7\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.0767590618336887\n",
      "Precision: 1.0\n",
      "Recall: 0.033482142857142856\n",
      "F1-Score: 0.06479481641468683\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.06073752711496746\n",
      "Precision: 1.0\n",
      "Recall: 0.015909090909090907\n",
      "F1-Score: 0.03131991051454139\n",
      "\n",
      "================== Other ==================\n",
      "\n",
      "False Positives: 0\n"
     ]
    }
   ],
   "source": [
    "analyze_predictions_YOLO_BirdNET(\"3_BirdNET_FineTuning_DetectorTest\", 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAhoar quiero modificar estas funciones:\\n\\ndef group_consecutive_predictions_birdnet(predictions_df):\\n    grouped_predictions = []\\n    current_group = []\\n    last_end = None\\n    for _, row in predictions_df.iterrows():\\n        if current_group and (row[\\'Start (s)\\'] != last_end or row[\\'Scientific name\\'] != current_group[-1][\\'Scientific name\\']):\\n            # New group starts here\\n            grouped_predictions.append(current_group)\\n            current_group = []\\n        current_group.append(row)\\n        last_end = row[\\'End (s)\\']\\n    if current_group:  # Add last group\\n        grouped_predictions.append(current_group)\\n    # Combine groups in unique predictions\\n    combined_predictions = []\\n    for group in grouped_predictions:\\n        combined_prediction = {\\n            \\'Start (s)\\': group[0][\\'Start (s)\\'],\\n            \\'End (s)\\': group[-1][\\'End (s)\\'],\\n            \\'Scientific name\\': group[0][\\'Scientific name\\'],\\n            \\'Confidence\\': max(item[\\'Confidence\\'] for item in group)  # conf = max confidence in group\\n        }\\n        combined_predictions.append(combined_prediction)\\n    return combined_predictions\\n\\n# Function to calculate the IoU\\ndef calculate_iou(interval1, interval2):\\n    start_max = max(interval1[0], interval2[0])\\n    end_min = min(interval1[1], interval2[1])\\n    intersection = max(0, end_min - start_max)\\n    union = (interval1[1] - interval1[0]) + (interval2[1] - interval2[0]) - intersection\\n    return intersection / union if union != 0 else 0\\n\\n# Function to check if a prediction is correct\\ndef is_prediction_correct_detector(prediction, gt_annotation, iou_threshold):\\n    iou = calculate_iou((prediction[\\'Start (s)\\'], prediction[\\'End (s)\\']), (gt_annotation[\\'start_time\\'], gt_annotation[\\'end_time\\']))\\n    return iou >= iou_threshold\\n\\n# Function to check if a prediction is correct\\ndef is_prediction_correct(prediction, gt_annotation, iou_threshold):\\n    iou = calculate_iou((prediction[\\'Start (s)\\'], prediction[\\'End (s)\\']), (gt_annotation[\\'start_time\\'], gt_annotation[\\'end_time\\']))\\n    prediction_class = prediction[\\'Scientific name\\'].lower()\\n    gt_class = gt_annotation[\\'specie\\'].lower()\\n    scientific_name_matches = prediction_class == gt_class\\n    return iou >= iou_threshold and scientific_name_matches\\n\\n    Y este codigo:\\n\\n    # Variables for metrics\\ncorrect_predictions = 0\\ntotal_predictions = 0\\ntotal_predictions_score = 0\\ncorrect_predictions_detector = 0\\niou_threshold = 0.4  # This value is editable\\nprediction_conf_score = 0.4  # This value is editable\\n\\n# Process the predictions\\nfor _, gt_annotation in gt_df.iterrows():\\n    prediction_path = PATH + f\"Dataset/BirdNET_Predictions/{gt_annotation[\\'path\\'].replace(\\'.WAV\\', \\'.BirdNET.results.csv\\')}\"\\n    try:\\n        predictions_df = pd.read_csv(prediction_path)\\n        grouped_predictions = group_consecutive_predictions_birdnet(predictions_df)  # Group predictions\\n        predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\\n\\n        for _, prediction in predictions_df.iterrows():\\n            total_predictions += 1\\n            if prediction[\\'Confidence\\'] >= prediction_conf_score:\\n                total_predictions_score += 1\\n                if is_prediction_correct(prediction, gt_annotation, iou_threshold):\\n                    correct_predictions += 1\\n                if is_prediction_correct_detector(prediction, gt_annotation, iou_threshold):\\n                    correct_predictions_detector += 1\\n    except FileNotFoundError:\\n        print(f\"Prediction file not found: {prediction_path}\")\\n\\n# Calculate and display the metrics\\nprint(\"================== Metrics ==================\\n\")\\nprint(f\"Total Predictions: {total_predictions}\")\\nprint(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\\nprint(f\"Total GT: {len(gt_df)}\")\\nprint(f\"Correct Predictions Detector: {correct_predictions_detector}\")\\nprint(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\\n\\n# Additional calculations for precision, recall, and F1-score\\nprint(\"\\n================== Detector Metrics ==================\\n\")\\ntrue_positives = correct_predictions_detector\\nfalse_positives = total_predictions_score - correct_predictions_detector\\nfalse_negatives = len(gt_df) - correct_predictions_detector\\ntrue_negatives = 0\\n\\n# Calculate and display the metrics\\naccuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\\nprecision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\\nrecall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\\nf1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\\nprint(f\"Accuracy: {accuracy}\")\\nprint(f\"Precision: {precision}\")\\nprint(f\"Recall: {recall}\")\\nprint(f\"F1-Score: {f1_score}\")\\n\\nprint(\"\\n================== Detector + Classifier Metrics ==================\\n\")\\ntrue_positives = correct_predictions\\nfalse_positives = total_predictions_score - correct_predictions\\nfalse_negatives = len(gt_df) - correct_predictions\\ntrue_negatives = 0\\n\\n# Calculate and display the metrics\\naccuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\\nprecision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\\nrecall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\\nf1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\\nprint(f\"Accuracy: {accuracy}\")\\nprint(f\"Precision: {precision}\")\\nprint(f\"Recall: {recall}\")\\nprint(f\"F1-Score: {f1_score}\")\\n\\nPara que funcione sobre el fichero predictions.json con la estructura de datos:\\n[{\"image_id\": \"AM4_20230531_110000\", \"category_id\": 0, \"bbox\": [301.605, 0.0, 31.041, 459.45], \"score\": 0.5276}, {\"image_id\": \"AM15_20230712_074000\", \"category_id\": 0, \"bbox\": [61.011, 0.439, 175.454, 460.228], \"score\": 0.48407}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [57.914, 0.573, 30.844, 461.131], \"score\": 0.65271}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [231.294, 1.093, 31.147, 460.902], \"score\": 0.53149}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [620.696, 2.401, 34.881, 459.599], \"score\": 0.44922}, {\"image_id\": \"AM8_20230304_093000\", \"category_id\": 0, \"bbox\": [765.516, 0.0, 162.609, 460.58], \"score\": 0.63678}, ...\\n\\nEn la que \\n'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacer con las predicciones de yolo como detectorr\n",
    "\n",
    "# Correr inference.py desde la VPN, intentar encontrar mejores valores de iou y conf y poner esos para las métricas aquí, luego coger esos valores y ponerlos\n",
    "\n",
    "# predictionms de val de yolov8:\n",
    "'''\n",
    "[{\"image_id\": \"AM4_20230531_110000\", \"category_id\": 0, \"bbox\": [301.605, 0.0, 31.041, 459.45], \"score\": 0.5276}, {\"image_id\": \"AM15_20230712_074000\", \"category_id\": 0, \"bbox\": [61.011, 0.439, 175.454, 460.228], \"score\": 0.48407}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [57.914, 0.573, 30.844, 461.131], \"score\": 0.65271}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [231.294, 1.093, 31.147, 460.902], \"score\": 0.53149}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [620.696, 2.401, 34.881, 459.599], \"score\": 0.44922}, {\"image_id\": \"AM8_20230304_093000\", \"category_id\": 0, \"bbox\": [765.516, 0.0, 162.609, 460.58], \"score\": 0.63678}, \n",
    "'''\n",
    "\n",
    "# Y si se hace predict sobre la carpeta test??\n",
    "\n",
    "\n",
    "# Copilot:\n",
    "'''\n",
    "Ahoar quiero modificar estas funciones:\n",
    "\n",
    "def group_consecutive_predictions_birdnet(predictions_df):\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    for _, row in predictions_df.iterrows():\n",
    "        if current_group and (row['Start (s)'] != last_end or row['Scientific name'] != current_group[-1]['Scientific name']):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['End (s)']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "    # Combine groups in unique predictions\n",
    "    combined_predictions = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'Start (s)': group[0]['Start (s)'],\n",
    "            'End (s)': group[-1]['End (s)'],\n",
    "            'Scientific name': group[0]['Scientific name'],\n",
    "            'Confidence': max(item['Confidence'] for item in group)  # conf = max confidence in group\n",
    "        }\n",
    "        combined_predictions.append(combined_prediction)\n",
    "    return combined_predictions\n",
    "\n",
    "# Function to calculate the IoU\n",
    "def calculate_iou(interval1, interval2):\n",
    "    start_max = max(interval1[0], interval2[0])\n",
    "    end_min = min(interval1[1], interval2[1])\n",
    "    intersection = max(0, end_min - start_max)\n",
    "    union = (interval1[1] - interval1[0]) + (interval2[1] - interval2[0]) - intersection\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct_detector(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    return iou >= iou_threshold\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    prediction_class = prediction['Scientific name'].lower()\n",
    "    gt_class = gt_annotation['specie'].lower()\n",
    "    scientific_name_matches = prediction_class == gt_class\n",
    "    return iou >= iou_threshold and scientific_name_matches\n",
    "\n",
    "    Y este codigo:\n",
    "\n",
    "    # Variables for metrics\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "total_predictions_score = 0\n",
    "correct_predictions_detector = 0\n",
    "iou_threshold = 0.4  # This value is editable\n",
    "prediction_conf_score = 0.4  # This value is editable\n",
    "\n",
    "# Process the predictions\n",
    "for _, gt_annotation in gt_df.iterrows():\n",
    "    prediction_path = PATH + f\"Dataset/BirdNET_Predictions/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "    try:\n",
    "        predictions_df = pd.read_csv(prediction_path)\n",
    "        grouped_predictions = group_consecutive_predictions_birdnet(predictions_df)  # Group predictions\n",
    "        predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "        for _, prediction in predictions_df.iterrows():\n",
    "            total_predictions += 1\n",
    "            if prediction['Confidence'] >= prediction_conf_score:\n",
    "                total_predictions_score += 1\n",
    "                if is_prediction_correct(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                if is_prediction_correct_detector(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions_detector += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "# Calculate and display the metrics\n",
    "print(\"================== Metrics ==================\\n\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "print(f\"Total GT: {len(gt_df)}\")\n",
    "print(f\"Correct Predictions Detector: {correct_predictions_detector}\")\n",
    "print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "# Additional calculations for precision, recall, and F1-score\n",
    "print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "true_positives = correct_predictions_detector\n",
    "false_positives = total_predictions_score - correct_predictions_detector\n",
    "false_negatives = len(gt_df) - correct_predictions_detector\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "true_positives = correct_predictions\n",
    "false_positives = total_predictions_score - correct_predictions\n",
    "false_negatives = len(gt_df) - correct_predictions\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "Para que funcione sobre el fichero predictions.json con la estructura de datos:\n",
    "[{\"image_id\": \"AM4_20230531_110000\", \"category_id\": 0, \"bbox\": [301.605, 0.0, 31.041, 459.45], \"score\": 0.5276}, {\"image_id\": \"AM15_20230712_074000\", \"category_id\": 0, \"bbox\": [61.011, 0.439, 175.454, 460.228], \"score\": 0.48407}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [57.914, 0.573, 30.844, 461.131], \"score\": 0.65271}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [231.294, 1.093, 31.147, 460.902], \"score\": 0.53149}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [620.696, 2.401, 34.881, 459.599], \"score\": 0.44922}, {\"image_id\": \"AM8_20230304_093000\", \"category_id\": 0, \"bbox\": [765.516, 0.0, 162.609, 460.58], \"score\": 0.63678}, ...\n",
    "\n",
    "En la que \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRDeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
