{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth CSV\n",
    "PATH = \"../Data/\"\n",
    "gt_csv = PATH + \"Dataset/CSVs/test_with_bg.csv\"\n",
    "\n",
    "# Read the Ground Truth CSV\n",
    "gt_df = pd.read_csv(gt_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_consecutive_predictions(predictions_df):\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    for _, row in predictions_df.iterrows():\n",
    "        if current_group and (row['Start (s)'] != last_end or row['Scientific name'] != current_group[-1]['Scientific name']):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['End (s)']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "    # Combine groups in unique predictions\n",
    "    combined_predictions = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'Start (s)': group[0]['Start (s)'],\n",
    "            'End (s)': group[-1]['End (s)'],\n",
    "            'Scientific name': group[0]['Scientific name'],\n",
    "            'Confidence': max(item['Confidence'] for item in group)  # conf = max confidence in group\n",
    "        }\n",
    "        combined_predictions.append(combined_prediction)\n",
    "    return combined_predictions\n",
    "\n",
    "# Function to calculate the IoU\n",
    "def calculate_iou(interval1, interval2):\n",
    "    start_max = max(interval1[0], interval2[0])\n",
    "    end_min = min(interval1[1], interval2[1])\n",
    "    intersection = max(0, end_min - start_max)\n",
    "    union = (interval1[1] - interval1[0]) + (interval2[1] - interval2[0]) - intersection\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct_detector_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    return iou >= iou_threshold\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    prediction_class = prediction['Scientific name'].lower()\n",
    "    gt_class = gt_annotation['specie'].lower()\n",
    "    scientific_name_matches = prediction_class == gt_class\n",
    "    return iou >= iou_threshold and scientific_name_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Base con la lista de especies de Doñana de BIRDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 2676\n",
      "Total Predictions with Score >= 0.6: 645\n",
      "Total GT: 542\n",
      "Correct Predictions Detector: 71\n",
      "Correct Predictions Detector + Classifier: 13\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.06362007168458782\n",
      "Precision: 0.11007751937984496\n",
      "Recall: 0.13099630996309963\n",
      "F1-Score: 0.11962931760741366\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.011073253833049404\n",
      "Precision: 0.020155038759689922\n",
      "Recall: 0.023985239852398525\n",
      "F1-Score: 0.02190395956192081\n"
     ]
    }
   ],
   "source": [
    "# Variables for metrics\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "total_predictions_score = 0\n",
    "correct_predictions_detector = 0\n",
    "iou_threshold = 0.1  # This value is editable\n",
    "prediction_conf_score = 0.6  # This value is editable\n",
    "\n",
    "# Process the predictions\n",
    "for _, gt_annotation in gt_df.iterrows():\n",
    "    prediction_path = f\"../BirdNET/Predictions/0_BirdNet_Base_AllTest_DonanaSpecies/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "    try:\n",
    "        predictions_df = pd.read_csv(prediction_path)\n",
    "        grouped_predictions = group_consecutive_predictions(predictions_df)  # Group predictions\n",
    "        predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "        for _, prediction in predictions_df.iterrows():\n",
    "            total_predictions += 1\n",
    "            if prediction['Confidence'] >= prediction_conf_score:\n",
    "                total_predictions_score += 1\n",
    "                if is_prediction_correct_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                if is_prediction_correct_detector_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions_detector += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "# Calculate and display the metrics\n",
    "print(\"================== Metrics ==================\\n\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "print(f\"Total GT: {len(gt_df)}\")\n",
    "print(f\"Correct Predictions Detector: {correct_predictions_detector}\")\n",
    "print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "# Additional calculations for precision, recall, and F1-score\n",
    "print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "true_positives = correct_predictions_detector\n",
    "false_positives = total_predictions_score - correct_predictions_detector\n",
    "false_negatives = len(gt_df) - correct_predictions_detector\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "true_positives = correct_predictions\n",
    "false_positives = total_predictions_score - correct_predictions\n",
    "false_negatives = len(gt_df) - correct_predictions\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de BirdNET Base (sin preentrenar) con la lista de especies del clasificador del finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 3300\n",
      "Total Predictions with Score >= 0.6: 907\n",
      "Total GT: 542\n",
      "Correct Predictions Detector: 99\n",
      "Correct Predictions Detector + Classifier: 29\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.07333333333333333\n",
      "Precision: 0.10915104740904079\n",
      "Recall: 0.18265682656826568\n",
      "F1-Score: 0.13664596273291926\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.020422535211267606\n",
      "Precision: 0.03197353914002205\n",
      "Recall: 0.05350553505535055\n",
      "F1-Score: 0.04002760524499655\n"
     ]
    }
   ],
   "source": [
    "# Variables for metrics\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "total_predictions_score = 0\n",
    "correct_predictions_detector = 0\n",
    "iou_threshold = 0.1  # This value is editable\n",
    "prediction_conf_score = 0.6  # This value is editable\n",
    "\n",
    "# Process the predictions\n",
    "for _, gt_annotation in gt_df.iterrows():\n",
    "    prediction_path = f\"../BirdNET/Predictions/1_BirdNet_Base_AllTest_ClassifierSpecies/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "    try:\n",
    "        predictions_df = pd.read_csv(prediction_path)\n",
    "        grouped_predictions = group_consecutive_predictions(predictions_df)  # Group predictions\n",
    "        predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "        for _, prediction in predictions_df.iterrows():\n",
    "            total_predictions += 1\n",
    "            if prediction['Confidence'] >= prediction_conf_score:\n",
    "                total_predictions_score += 1\n",
    "                if is_prediction_correct_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                if is_prediction_correct_detector_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions_detector += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "# Calculate and display the metrics\n",
    "print(\"================== Metrics ==================\\n\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "print(f\"Total GT: {len(gt_df)}\")\n",
    "print(f\"Correct Predictions Detector: {correct_predictions_detector}\")\n",
    "print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "# Additional calculations for precision, recall, and F1-score\n",
    "print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "true_positives = correct_predictions_detector\n",
    "false_positives = total_predictions_score - correct_predictions_detector\n",
    "false_negatives = len(gt_df) - correct_predictions_detector\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "true_positives = correct_predictions\n",
    "false_positives = total_predictions_score - correct_predictions\n",
    "false_negatives = len(gt_df) - correct_predictions\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer con las predicciones de yolo como detectorr\n",
    "\n",
    "# Correr inference.py desde la VPN, intentar encontrar mejores valores de iou y conf y poner esos para las métricas aquí, luego coger esos valores y ponerlos\n",
    "\n",
    "# predictionms de val de yolov8:\n",
    "'''\n",
    "[{\"image_id\": \"AM4_20230531_110000\", \"category_id\": 0, \"bbox\": [301.605, 0.0, 31.041, 459.45], \"score\": 0.5276}, {\"image_id\": \"AM15_20230712_074000\", \"category_id\": 0, \"bbox\": [61.011, 0.439, 175.454, 460.228], \"score\": 0.48407}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [57.914, 0.573, 30.844, 461.131], \"score\": 0.65271}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [231.294, 1.093, 31.147, 460.902], \"score\": 0.53149}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [620.696, 2.401, 34.881, 459.599], \"score\": 0.44922}, {\"image_id\": \"AM8_20230304_093000\", \"category_id\": 0, \"bbox\": [765.516, 0.0, 162.609, 460.58], \"score\": 0.63678}, \n",
    "'''\n",
    "\n",
    "# Y si se hace predict sobre la carpeta test??\n",
    "\n",
    "\n",
    "# Copilot:\n",
    "'''\n",
    "Ahoar quiero modificar estas funciones:\n",
    "\n",
    "def group_consecutive_predictions(predictions_df):\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    for _, row in predictions_df.iterrows():\n",
    "        if current_group and (row['Start (s)'] != last_end or row['Scientific name'] != current_group[-1]['Scientific name']):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['End (s)']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "    # Combine groups in unique predictions\n",
    "    combined_predictions = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'Start (s)': group[0]['Start (s)'],\n",
    "            'End (s)': group[-1]['End (s)'],\n",
    "            'Scientific name': group[0]['Scientific name'],\n",
    "            'Confidence': max(item['Confidence'] for item in group)  # conf = max confidence in group\n",
    "        }\n",
    "        combined_predictions.append(combined_prediction)\n",
    "    return combined_predictions\n",
    "\n",
    "# Function to calculate the IoU\n",
    "def calculate_iou(interval1, interval2):\n",
    "    start_max = max(interval1[0], interval2[0])\n",
    "    end_min = min(interval1[1], interval2[1])\n",
    "    intersection = max(0, end_min - start_max)\n",
    "    union = (interval1[1] - interval1[0]) + (interval2[1] - interval2[0]) - intersection\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct_detector(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    return iou >= iou_threshold\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    prediction_class = prediction['Scientific name'].lower()\n",
    "    gt_class = gt_annotation['specie'].lower()\n",
    "    scientific_name_matches = prediction_class == gt_class\n",
    "    return iou >= iou_threshold and scientific_name_matches\n",
    "\n",
    "    Y este codigo:\n",
    "\n",
    "    # Variables for metrics\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "total_predictions_score = 0\n",
    "correct_predictions_detector = 0\n",
    "iou_threshold = 0.4  # This value is editable\n",
    "prediction_conf_score = 0.4  # This value is editable\n",
    "\n",
    "# Process the predictions\n",
    "for _, gt_annotation in gt_df.iterrows():\n",
    "    prediction_path = PATH + f\"Dataset/BirdNET_Predictions/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "    try:\n",
    "        predictions_df = pd.read_csv(prediction_path)\n",
    "        grouped_predictions = group_consecutive_predictions(predictions_df)  # Group predictions\n",
    "        predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "        for _, prediction in predictions_df.iterrows():\n",
    "            total_predictions += 1\n",
    "            if prediction['Confidence'] >= prediction_conf_score:\n",
    "                total_predictions_score += 1\n",
    "                if is_prediction_correct(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                if is_prediction_correct_detector(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions_detector += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "# Calculate and display the metrics\n",
    "print(\"================== Metrics ==================\\n\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "print(f\"Total GT: {len(gt_df)}\")\n",
    "print(f\"Correct Predictions Detector: {correct_predictions_detector}\")\n",
    "print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "# Additional calculations for precision, recall, and F1-score\n",
    "print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "true_positives = correct_predictions_detector\n",
    "false_positives = total_predictions_score - correct_predictions_detector\n",
    "false_negatives = len(gt_df) - correct_predictions_detector\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "true_positives = correct_predictions\n",
    "false_positives = total_predictions_score - correct_predictions\n",
    "false_negatives = len(gt_df) - correct_predictions\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "Para que funcione sobre el fichero predictions.json con la estructura de datos:\n",
    "[{\"image_id\": \"AM4_20230531_110000\", \"category_id\": 0, \"bbox\": [301.605, 0.0, 31.041, 459.45], \"score\": 0.5276}, {\"image_id\": \"AM15_20230712_074000\", \"category_id\": 0, \"bbox\": [61.011, 0.439, 175.454, 460.228], \"score\": 0.48407}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [57.914, 0.573, 30.844, 461.131], \"score\": 0.65271}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [231.294, 1.093, 31.147, 460.902], \"score\": 0.53149}, {\"image_id\": \"AM15_20230330_070000\", \"category_id\": 0, \"bbox\": [620.696, 2.401, 34.881, 459.599], \"score\": 0.44922}, {\"image_id\": \"AM8_20230304_093000\", \"category_id\": 0, \"bbox\": [765.516, 0.0, 162.609, 460.58], \"score\": 0.63678}, ...\n",
    "\n",
    "En la que \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de Test sobre BirdNET preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 9596\n",
      "Total Predictions with Score >= 0.6: 229\n",
      "Total GT: 542\n",
      "Correct Predictions Detector: 30\n",
      "Correct Predictions Detector + Classifier: 12\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.04048582995951417\n",
      "Precision: 0.13100436681222707\n",
      "Recall: 0.055350553505535055\n",
      "F1-Score: 0.07782101167315175\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.015810276679841896\n",
      "Precision: 0.05240174672489083\n",
      "Recall: 0.02214022140221402\n",
      "F1-Score: 0.0311284046692607\n"
     ]
    }
   ],
   "source": [
    "# Hacer con las predicciones de BirdNet después de haber entrenado sobre el conjunto de test total\n",
    "# Variables for metrics\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "total_predictions_score = 0\n",
    "correct_predictions_detector = 0\n",
    "iou_threshold = 0.1  # This value is editable\n",
    "prediction_conf_score = 0.6  # This value is editable\n",
    "\n",
    "# Process the predictions\n",
    "for _, gt_annotation in gt_df.iterrows():\n",
    "    prediction_path = f\"../BirdNET/Predictions/2_BirdNet_FineTuning_AllTest/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "    try:\n",
    "        predictions_df = pd.read_csv(prediction_path)\n",
    "        grouped_predictions = group_consecutive_predictions(predictions_df)  # Group predictions\n",
    "        predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "        for _, prediction in predictions_df.iterrows():\n",
    "            total_predictions += 1\n",
    "            if prediction['Confidence'] >= prediction_conf_score:\n",
    "                total_predictions_score += 1\n",
    "                if is_prediction_correct_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                if is_prediction_correct_detector_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions_detector += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "# Calculate and display the metrics\n",
    "print(\"================== Metrics ==================\\n\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "print(f\"Total GT: {len(gt_df)}\")\n",
    "print(f\"Correct Predictions Detector: {correct_predictions_detector}\")\n",
    "print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "# Additional calculations for precision, recall, and F1-score\n",
    "print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "true_positives = correct_predictions_detector\n",
    "false_positives = total_predictions_score - correct_predictions_detector\n",
    "false_negatives = len(gt_df) - correct_predictions_detector\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "true_positives = correct_predictions\n",
    "false_positives = total_predictions_score - correct_predictions\n",
    "false_negatives = len(gt_df) - correct_predictions\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas sobre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer con las predicciones de BIRDNET despues de entrenar sobre los recortes del detector de yolov8\n",
    "\n",
    "# Hacer con las predicciones de BirdNet después de haber entrenado sobre el conjunto de test total\n",
    "# Variables for metrics\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "total_predictions_score = 0\n",
    "correct_predictions_detector = 0\n",
    "iou_threshold = 0.1  # This value is editable\n",
    "prediction_conf_score = 0.6  # This value is editable\n",
    "\n",
    "# Process the predictions\n",
    "for _, gt_annotation in gt_df.iterrows():\n",
    "    prediction_path = f\"../BirdNET/Predictions/3_BirdNET_FineTuning_DetectorTest/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "    try:\n",
    "        predictions_df = pd.read_csv(prediction_path)\n",
    "        grouped_predictions = group_consecutive_predictions(predictions_df)  # Group predictions\n",
    "        predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "        for _, prediction in predictions_df.iterrows():\n",
    "            total_predictions += 1\n",
    "            if prediction['Confidence'] >= prediction_conf_score:\n",
    "                total_predictions_score += 1\n",
    "                if is_prediction_correct_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                if is_prediction_correct_detector_birdnet(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions_detector += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "# Calculate and display the metrics\n",
    "print(\"================== Metrics ==================\\n\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "print(f\"Total GT: {len(gt_df)}\")\n",
    "print(f\"Correct Predictions Detector: {correct_predictions_detector}\")\n",
    "print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "# Additional calculations for precision, recall, and F1-score\n",
    "print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "true_positives = correct_predictions_detector\n",
    "false_positives = total_predictions_score - correct_predictions_detector\n",
    "false_negatives = len(gt_df) - correct_predictions_detector\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "true_positives = correct_predictions\n",
    "false_positives = total_predictions_score - correct_predictions\n",
    "false_negatives = len(gt_df) - correct_predictions\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRDeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
