{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth CSV\n",
    "PATH = \"../Data/\"\n",
    "gt_csv = PATH + \"Dataset/CSVs/test_with_bg.csv\"\n",
    "\n",
    "# Read the Ground Truth CSV\n",
    "gt_df = pd.read_csv(gt_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_consecutive_predictions(predictions_df):\n",
    "    grouped_predictions = []\n",
    "    current_group = []\n",
    "    last_end = None\n",
    "    for _, row in predictions_df.iterrows():\n",
    "        if current_group and (row['Start (s)'] != last_end or row['Scientific name'] != current_group[-1]['Scientific name']):\n",
    "            # New group starts here\n",
    "            grouped_predictions.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append(row)\n",
    "        last_end = row['End (s)']\n",
    "    if current_group:  # Add last group\n",
    "        grouped_predictions.append(current_group)\n",
    "    # Combine groups in unique predictions\n",
    "    combined_predictions = []\n",
    "    for group in grouped_predictions:\n",
    "        combined_prediction = {\n",
    "            'Start (s)': group[0]['Start (s)'],\n",
    "            'End (s)': group[-1]['End (s)'],\n",
    "            'Scientific name': group[0]['Scientific name'],\n",
    "            'Confidence': max(item['Confidence'] for item in group)  # conf = max confidence in group\n",
    "        }\n",
    "        combined_predictions.append(combined_prediction)\n",
    "    return combined_predictions\n",
    "\n",
    "# Function to calculate the IoU\n",
    "def calculate_iou(interval1, interval2):\n",
    "    start_max = max(interval1[0], interval2[0])\n",
    "    end_min = min(interval1[1], interval2[1])\n",
    "    intersection = max(0, end_min - start_max)\n",
    "    union = (interval1[1] - interval1[0]) + (interval2[1] - interval2[0]) - intersection\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct_detector(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    return iou >= iou_threshold\n",
    "\n",
    "# Function to check if a prediction is correct\n",
    "def is_prediction_correct(prediction, gt_annotation, iou_threshold):\n",
    "    iou = calculate_iou((prediction['Start (s)'], prediction['End (s)']), (gt_annotation['start_time'], gt_annotation['end_time']))\n",
    "    prediction_class = prediction['Scientific name'].lower()\n",
    "    gt_class = gt_annotation['specie'].lower()\n",
    "    scientific_name_matches = prediction_class == gt_class\n",
    "    return iou >= iou_threshold and scientific_name_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Metrics ==================\n",
      "\n",
      "Total Predictions: 2303\n",
      "Total Predictions with Score >= 0.8: 1016\n",
      "Total GT: 542\n",
      "Correct Predictions Detector: 22\n",
      "Correct Predictions Detector + Classifier: 4\n",
      "\n",
      "================== Detector Metrics ==================\n",
      "\n",
      "Accuracy: 0.014322916666666666\n",
      "Precision: 0.021653543307086614\n",
      "Recall: 0.04059040590405904\n",
      "F1-Score: 0.0282413350449294\n",
      "\n",
      "================== Detector + Classifier Metrics ==================\n",
      "\n",
      "Accuracy: 0.002574002574002574\n",
      "Precision: 0.003937007874015748\n",
      "Recall: 0.007380073800738007\n",
      "F1-Score: 0.005134788189987163\n"
     ]
    }
   ],
   "source": [
    "# Variables for metrics\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "total_predictions_score = 0\n",
    "correct_predictions_detector = 0\n",
    "iou_threshold = 0.4  # This value is editable\n",
    "prediction_conf_score = 0.8  # This value is editable\n",
    "\n",
    "# Process the predictions\n",
    "for _, gt_annotation in gt_df.iterrows():\n",
    "    prediction_path = PATH + f\"Dataset/BirdNET_Predictions/{gt_annotation['path'].replace('.WAV', '.BirdNET.results.csv')}\"\n",
    "    try:\n",
    "        predictions_df = pd.read_csv(prediction_path)\n",
    "        grouped_predictions = group_consecutive_predictions(predictions_df)  # Group predictions\n",
    "        predictions_df = pd.DataFrame(grouped_predictions)  # Convert list dict to DataFrame\n",
    "\n",
    "        for _, prediction in predictions_df.iterrows():\n",
    "            total_predictions += 1\n",
    "            if prediction['Confidence'] >= prediction_conf_score:\n",
    "                total_predictions_score += 1\n",
    "                if is_prediction_correct(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions += 1\n",
    "                if is_prediction_correct_detector(prediction, gt_annotation, iou_threshold):\n",
    "                    correct_predictions_detector += 1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Prediction file not found: {prediction_path}\")\n",
    "\n",
    "# Calculate and display the metrics\n",
    "print(\"================== Metrics ==================\\n\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Predictions with Score >= {prediction_conf_score}: {total_predictions_score}\")\n",
    "print(f\"Total GT: {len(gt_df)}\")\n",
    "print(f\"Correct Predictions Detector: {correct_predictions_detector}\")\n",
    "print(f\"Correct Predictions Detector + Classifier: {correct_predictions}\")\n",
    "\n",
    "# Additional calculations for precision, recall, and F1-score\n",
    "print(\"\\n================== Detector Metrics ==================\\n\")\n",
    "true_positives = correct_predictions_detector\n",
    "false_positives = total_predictions_score - correct_predictions_detector\n",
    "false_negatives = len(gt_df) - correct_predictions_detector\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "\n",
    "print(\"\\n================== Detector + Classifier Metrics ==================\\n\")\n",
    "true_positives = correct_predictions\n",
    "false_positives = total_predictions_score - correct_predictions\n",
    "false_negatives = len(gt_df) - correct_predictions\n",
    "true_negatives = 0\n",
    "\n",
    "# Calculate and display the metrics\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives) if true_positives + true_negatives + false_positives + false_negatives != 0 else 0\n",
    "precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer con las predicciones de yolo como detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer con las predicciones de BirdNet despu√©s de haber entrenado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRDeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
