{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings Extraction for Bird Vocalizations Classification\n",
    "\n",
    "This notebook extracts 1024-dimensional audio embeddings from a fine-tuned BirdNET model. It processes 3-second audio segments, feeds them into the model, and retrieves feature representations from an intermediate layer. These embeddings can be used for training traditional machine learning models like Random Forest bird species classification.\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BirdNET Fine Tuned Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../models/BirdNET_CustomClassifier/2025_CustomClassifier_DF_REPEAT_025_MIXUP_SEGMENTS.tflite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the TFLite model\n",
    "# interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# # Get layer details\n",
    "# layer_details = interpreter.get_tensor_details()\n",
    "\n",
    "# # Print layer details\n",
    "# for layer in layer_details:\n",
    "#     print(\"Layer Name:\", layer['name'])\n",
    "#     print(\"Layer Index:\", layer['index'])\n",
    "#     print(\"Layer Shape:\", layer['shape'])\n",
    "#     print(\"Layer Type:\", layer['dtype'])\n",
    "#     print(\"Quantization Parameters:\", layer['quantization'])\n",
    "#     print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (1, 1024)\n",
      "Embedding vector: [[0.         0.37047914 0.32140145 ... 0.98294234 0.0562969  1.4812535 ]]\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Obtener los índices de entrada y salida\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Configurar los índices de la capa de embedding (en este caso, la capa GlobalAveragePooling)\n",
    "embedding_index = 545 #547  # Índice de model/GLOBAL_AVG_POOL/Mean, como identificaste\n",
    "\n",
    "# Asignar una entrada de prueba\n",
    "input_data = np.random.random(size=input_details[0]['shape']).astype(np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Ejecutar la inferencia\n",
    "interpreter.invoke()\n",
    "\n",
    "# Extraer el embedding\n",
    "embedding = interpreter.get_tensor(embedding_index)\n",
    "print(\"Embedding shape:\", embedding.shape)\n",
    "print(\"Embedding vector:\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings shape is 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'StatefulPartitionedCall:0', 'index': 546, 'shape': array([ 1, 28], dtype=int32), 'shape_signature': array([-1, 28], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "# Load tflite model\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "for detail in interpreter.get_output_details():\n",
    "    print(detail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tflite model\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Audio Config\n",
    "target_sample_rate = 48000\n",
    "target_duration = 3  # Target Duration in sefcomds\n",
    "target_length = target_sample_rate * target_duration  # Frame legth of 3 seconds\n",
    "\n",
    "# Función para preprocesar cada audio\n",
    "def preprocess_audio(audio_path):\n",
    "    audio, sr = librosa.load(audio_path)\n",
    "    audios = []\n",
    "    segment_idx = []\n",
    "\n",
    "    if sr != target_sample_rate:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sample_rate)\n",
    "\n",
    "    if len(audio) > target_length: # segment in as many audios as possible\n",
    "        for i in range(0, len(audio) - target_length, target_length):\n",
    "            audios.append(audio[i:i + target_length])\n",
    "            segment_idx.append(int(i / target_length))\n",
    "        return audios, segment_idx\n",
    "    else:\n",
    "        padding = target_length - len(audio)\n",
    "        audio = np.pad(audio, (padding // 2, padding - padding // 2), 'constant')\n",
    "        audios.append(audio)\n",
    "        segment_idx.append(0)\n",
    "    return audios, segment_idx\n",
    "\n",
    "# Función para obtener el embedding desde el modelo TFLite\n",
    "def get_embedding(audio_data):\n",
    "    input_shape = input_details[0]['shape']\n",
    "    audio_data = np.reshape(audio_data, input_shape).astype(np.float32)\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0]['index'], audio_data)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Obtener el embedding de la capa con index 545 (tamaño 1024)\n",
    "    embedding_index = 545  # Índice de la capa deseada\n",
    "    embedding = interpreter.get_tensor(embedding_index)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "    # print(f\"Embedding shape: {embedding.shape}\")  # Confirmar el tamaño (1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener embeddings y etiquetas del conjunto de entrenamiento\n",
    "train_folder = \"../../../Data/Dataset/Audios/For Classifier/train\"\n",
    "train_embeddings = []\n",
    "train_labels = []\n",
    "\n",
    "for label_folder in os.listdir(train_folder):\n",
    "    label_path = os.path.join(train_folder, label_folder)\n",
    "    if os.path.isdir(label_path):\n",
    "        for file_name in os.listdir(label_path):\n",
    "            if file_name.endswith(\".WAV\") or file_name.endswith(\".wav\"):\n",
    "                audio_path = os.path.join(label_path, file_name)\n",
    "                audio_data, _ = preprocess_audio(audio_path)\n",
    "                for audio in audio_data:\n",
    "                    embedding = get_embedding(audio)\n",
    "                    train_embeddings.append(embedding)\n",
    "                    train_labels.append(label_folder)  # La etiqueta es el nombre de la carpeta\n",
    "\n",
    "train_embeddings = np.array(train_embeddings)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Save embeddings and labels in Dataset/Embeddings\n",
    "np.save(\"../../../Data/Dataset/Embeddings/train_embeddings.npy\", train_embeddings)\n",
    "np.save(\"../../../Data/Dataset/Embeddings/train_labels.npy\", train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener embeddings y etiquetas del conjunto de entrenamiento\n",
    "validation_folder = \"../../../Data/Dataset/Audios/For Classifier/validation\"\n",
    "validation_embeddings = []\n",
    "validation_labels = []\n",
    "\n",
    "for label_folder in os.listdir(validation_folder):\n",
    "    label_path = os.path.join(validation_folder, label_folder)\n",
    "    if os.path.isdir(label_path):\n",
    "        for file_name in os.listdir(label_path):\n",
    "            if file_name.endswith(\".WAV\") or file_name.endswith(\".wav\"):\n",
    "                audio_path = os.path.join(label_path, file_name)\n",
    "                audio_data, _ = preprocess_audio(audio_path)\n",
    "                for audio in audio_data:\n",
    "                    embedding = get_embedding(audio)\n",
    "                    validation_embeddings.append(embedding)\n",
    "                    validation_labels.append(label_folder)  # La etiqueta es el nombre de la carpeta\n",
    "\n",
    "validation_embeddings = np.array(validation_embeddings)\n",
    "validation_labels = np.array(validation_labels)\n",
    "\n",
    "# Save embeddings and labels in Dataset/Embeddings\n",
    "np.save(\"../../../Data/Dataset/Embeddings/validation_embeddings.npy\", validation_embeddings)\n",
    "np.save(\"../../../Data/Dataset/Embeddings/validation_labels.npy\", validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Embeddings\n",
    "\n",
    "#### Full Audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener embeddings y etiquetas del conjunto de entrenamiento\n",
    "test_folder = \"../../../Data/Dataset/Audios/For Classifier/test\"\n",
    "test_embeddings = []\n",
    "test_file_names = []\n",
    "test_file_idx = []\n",
    "\n",
    "# walk in subdirectories \n",
    "for root, dirs, files in os.walk(test_folder):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith(\".WAV\") or file_name.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(root, file_name)\n",
    "            audio_data, files_idx = preprocess_audio(audio_path)\n",
    "            for audio, file_idx in zip(audio_data, files_idx):\n",
    "                embedding = get_embedding(audio)\n",
    "                test_embeddings.append(embedding)\n",
    "                test_file_names.append(file_name)  # La etiqueta es el nombre de la carpeta\n",
    "                test_file_idx.append(file_idx)\n",
    "\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "test_file_names = np.array(test_file_names)\n",
    "test_file_idx = np.array(test_file_idx)\n",
    "\n",
    "# Save embeddings and labels in Dataset/Embeddings\n",
    "np.save(\"../../../Data/Dataset/Embeddings/test_embeddings.npy\", test_embeddings)\n",
    "np.save(\"../../../Data/Dataset/Embeddings/test_file_names.npy\", test_file_names)\n",
    "np.save(\"../../../Data/Dataset/Embeddings/test_file_idx.npy\", test_file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bird Song Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener embeddings y etiquetas del conjunto de entrenamiento\n",
    "test_folder = \"../../../Data/Dataset/Audios/For Classifier/BirdSongDetectorTestSegments\"\n",
    "bsd_test_embeddings = []\n",
    "bsd_test_file_names = []\n",
    "bsd_files_idx = []\n",
    "\n",
    "# walk in subdirectories \n",
    "for file_name in os.listdir(test_folder):\n",
    "    if file_name.endswith(\".WAV\") or file_name.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(test_folder, file_name)\n",
    "        audio_data, files_idx = preprocess_audio(audio_path)\n",
    "        for audio, file_idx in zip(audio_data, files_idx):\n",
    "            embedding = get_embedding(audio)\n",
    "            bsd_test_embeddings.append(embedding)\n",
    "            bsd_test_file_names.append(file_name)  # La etiqueta es el nombre de la carpeta\n",
    "            bsd_files_idx.append(file_idx)\n",
    "\n",
    "bsd_test_embeddings = np.array(bsd_test_embeddings)\n",
    "bsd_test_file_names = np.array(bsd_test_file_names)\n",
    "bsd_files_idx = np.array(bsd_files_idx)\n",
    "\n",
    "# Save embeddings and labels in Dataset/Embeddings\n",
    "np.save(\"../../../Data/Dataset/Embeddings/bsd_test_embeddings.npy\", bsd_test_embeddings)\n",
    "np.save(\"../../../Data/Dataset/Embeddings/bsd_test_file_names.npy\", bsd_test_file_names)\n",
    "np.save(\"../../../Data/Dataset/Embeddings/bsd_files_idx.npy\", bsd_files_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Embeddings Shape: (3739, 1, 1024)\n",
      "Validation Embeddings Shape: (985, 1, 1024)\n",
      "Test Embeddings Shape: (1840, 1, 1024)\n",
      "BSD Test Embeddings Shape: (329, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "# check size of embeddings\n",
    "print(\"Train Embeddings Shape:\", train_embeddings.shape)\n",
    "print(\"Validation Embeddings Shape:\", validation_embeddings.shape)\n",
    "print(\"Test Embeddings Shape:\", test_embeddings.shape)\n",
    "print(\"BSD Test Embeddings Shape:\", bsd_test_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BirdNET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
